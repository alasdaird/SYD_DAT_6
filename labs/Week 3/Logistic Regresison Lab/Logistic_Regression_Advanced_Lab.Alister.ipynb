{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (optional advanced lab)\n",
    "\n",
    "Logistic regression models $P(y|\\boldsymbol{x})$ directly by assuming it is a (logistic) function of a linear combination of the features. The logistic function $\\theta(s) = \\frac{e^s}{e^s+1}$ maps the weighted features to $[0,1]$ to allow it to model a probability. Training logistic regression corresponds to learning the weights $\\boldsymbol{w}$ to maximise the likelihood function:\n",
    "\n",
    "\\begin{equation}\n",
    "P(y_1...y_n|\\boldsymbol{x}_1...\\boldsymbol{x}_n,\\boldsymbol{w}) = \\prod_{i=1}^n \\theta(y_i\\boldsymbol{w}^T\\boldsymbol{x}_i)\n",
    "\\end{equation}\n",
    "\n",
    "Maximising the likelihood $P(y_1...y_n|\\boldsymbol{x}_1...\\boldsymbol{x}_n,\\boldsymbol{w})$ is equivalent to minimising the negative log-likelihood: \n",
    "\\begin{equation}\n",
    "\\boldsymbol{w}^* = argmin_{\\boldsymbol{w}}\\left( -\\log\\left(\\prod_{i=1}^n \\theta(y_i\\boldsymbol{w}^T\\boldsymbol{x}_i)\\right)\\right)\n",
    "= argmin_{\\boldsymbol{w}}\\left( \\sum_{i=1}^n \\ln(1+e^{-y_i\\boldsymbol{w}^T\\boldsymbol{x}_i})\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Once we have the weights $\\boldsymbol{w}^*$, we can predict the probability that a new observation belongs to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install python-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import python_utils\n",
    "\n",
    "from scipy.stats import itemfreq\n",
    "from scipy.stats import bernoulli\n",
    "from scipy.stats import multivariate_normal as mvnorm\n",
    "\n",
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib\n",
    "print(\"SKLEARN\",sklearn.__version__)\n",
    "print (\"SCIPY\",scipy.version.full_version)\n",
    "print(\"NUMPY\",np.__version__)\n",
    "print(\"MATPLOTLIB\",matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise: Use Sklearn to fit a logistic regression model on the gaussian mixture data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generates some data from a Gaussian Mixture Model. \n",
    "mean0 = [-1,-1]  # the mean of the gaussian for class 0      \n",
    "mean1 = [1,1] # the mean of the gaussian for class 1\n",
    "cov0 = [[.5, .28], [.28, .5]] # the covariance matrix for class 0\n",
    "cov1 = [[1, -.8], [-.8, 1]] # the covariance matrix for class 1\n",
    "mixture = util.GaussianMixture(mean0,cov0,mean1,cov1)\n",
    "mX,mY = mixture.sample(500,0.5,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run Logistic regression on the gaussian mixture data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic = YOUR CODE HERE # create and fit a logistic regression model on mX,mY\n",
    "\n",
    "# print out the intercept and coefficients, w\n",
    "YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the probability y = 1 as over the feature space as for Naive Bayes\n",
    "logistz = logistic.predict_proba(test_points)[:,1].reshape(len(x),len(y)) # probability Y = 1\n",
    "f,ax = subplots(1,1,figsize=(5,5))\n",
    "cn = ax.contourf(x,y,logistz)\n",
    "ct = ax.contour(cn,levels=[0.5])\n",
    "ax.scatter(mX[:,0],mX[:,1],s=5, c = [\"black\" if t < 1 else \"white\" for t in mY],alpha=1)\n",
    "ax.clabel(ct)\n",
    "show()# implement the jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can model more complex decision boundaries by expanding the feature space to include combinations of features\n",
    "\n",
    "# re-fit logistic regression adding in all quadratic combinations of features ie x1,x2,x1x2,x1^2,x2^2\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_expand = YOUR CODE HERE # create a polynomial feature transformer that produces quadratic combinations\n",
    "m2X = YOUR CODE HERE # use poly_expand to transform the original features (mX)\n",
    "logistic.YOUR CODE HERE # fit the logistic model with the new features\n",
    "\n",
    "# transform the test plots and predict and plot\n",
    "testpoints2 = poly_expand.transform(test_points)\n",
    "logistic2z = logistic.predict_proba(testpoints2)[:,1].reshape(len(x),len(y)) # probability Y = 1\n",
    "f,ax = subplots(1,1,figsize=(5,5))\n",
    "cn = ax.contourf(x,y,logistic2z)\n",
    "ct = ax.contour(cn,levels=[0.5])\n",
    "ax.scatter(mX[:,0],mX[:,1],s=5, c = [\"black\" if t < 1 else \"white\" for t in mY],alpha=1)\n",
    "ax.clabel(ct)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With large numbers of features there is a risk of over fitting to the training data. We can tune a logistic regression model to reduce the risk of over fitting by penalising large weights, $\\boldsymbol{w}$ \n",
    "\n",
    "*Exersise: Experiment with the regularisation parameters sklearn provides: \n",
    "penalty = \"l1\" or \"l2\" and C = inverse of weight of regularisation term *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lreg = YOUR CODE HERE # create and fit a logistic regression model to the quadraticly expanded features\n",
    "\n",
    "\n",
    "# plots the probability as before\n",
    "logistic2z_reg = lreg.predict_proba(testpoints2)[:,1].reshape(len(x),len(y)) # probability Y = 1\n",
    "f,ax = subplots(1,1,figsize=(5,5))\n",
    "cn = ax.contourf(x,y,logistic2z_reg)\n",
    "ct = ax.contour(cn,levels=[0.5])\n",
    "ax.scatter(mX[:,0],mX[:,1],s=5, c = [\"black\" if t < 1 else \"white\" for t in mY],alpha=1)\n",
    "ax.clabel(ct)\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
