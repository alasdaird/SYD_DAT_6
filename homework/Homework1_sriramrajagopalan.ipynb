{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYD DAT 6 Lab 1 - Git and Markdown\n",
    "\n",
    "## Homework:\n",
    "\n",
    "#### Setup\n",
    "* Resolve any installation issues before next class.\n",
    "* Make sure you have a github profile and created a repo called \"SYD_DAT_6\"\n",
    "* Clone the class repo (this one!)\n",
    "* Review this [code](../labs/Week 1/00_python_refresher.py) for a recap of some Python basics.\n",
    "\n",
    "#### Communication\n",
    "* Read [Analyzing the Analyzers](http://cdn.oreillystatic.com/oreilly/radarreport/0636920029014/Analyzing_the_Analyzers.pdf) for a useful look at the different types of data scientists. Write down 5 key points you took away from the article\n",
    "* Read about some [Markdown Techniques](http://daringfireball.net/projects/markdown/syntax)\n",
    "* Write a summary of 2 chapters of [The Data Science Handbook](http://www.thedatasciencehandbook.com/) in Markdown and submit a pull request in the Lab Directory\n",
    "\n",
    "#### Programming\n",
    "* Complete the lab from class and the additional Exercise below\n",
    "\n",
    "#### Course Project\n",
    "* Come up with 5 different ideas for your course project. For each one list:\n",
    "  * Overview of your idea\n",
    "  * What data you will use\n",
    "  * What the outcome is that you are trying to achieve\n",
    "  * Any ideas of modelling techniques it may involve\n",
    "\n",
    "**Instructions: copy this file and append your name in the filename, e.g. Homework1_alasdair_douglas.ipynb.\n",
    "Then commit this in your local repository, push it to your github account and create a pull request so I can see your work. Remeber if you get stuck to look at the slides going over Fork, Clone, Commit, Push and Pull request.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Science Handbook Summary\n",
    "\n",
    "The Data Science Handbook is a collection of interview transcripts with Data Science professionals covering topics such as career paths, skills & challenges they have faced in the industry.\n",
    "\n",
    "DJ Patil provides into the challenges one faces entering the job market from university and how one must market themselves. He further demonstrates the value and lessons one must learn from failures. He further delves into the importance of team work in the industry.\n",
    "\n",
    "Clare Corthwell describes and discusses the origins of Open Source Data Science masters, her personal start up which provides an open source of learning for budding data scientists and allows them to gain the necessary skills to be entry level data scientists. Clare further emphasises the soft skills, technical skills and attitude one must have when interviewing for data science positions.\n",
    "\n",
    "Michelangelo discusses the benefits of having an academic (PHD) background in the data science field. Michelangelo highlights the most important skill one must have - the ability to learn independently and stick to solving a problem for as long as possible. Always \"have a go\" at solving the problem. Michelangelo further provides advice to those looking to transition from graduate school into the data science industry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the Analysers.\n",
    "\n",
    "Point 1 - The article organises data scientists into 4 base catageories\n",
    "    Data Developer - organise and structure and storage of data\n",
    "    Data Creative - \"jack of all trades\", tends to not specialise in any one area\n",
    "    Data Researcher - skills in mathematics/statistics\n",
    "    Data Business - skills in business management\n",
    "\n",
    "Point 2 - Very rarely do data scientists deal with data equal to or greater than a tera byte- in size. Even analysis of 1 GB tends to be uncommon\n",
    "\n",
    "point 3 - Programming is a necessary base requirement for data scientists. Those who donot use tools such as excel\n",
    "donot fit into the data scientists categories \"analysts that can't program are disenfranchised here\"\n",
    "\n",
    "point 4 - Most successful data scientists are \"T-shaped\", broad range of skills accross many areas, with solid depth \n",
    "in one particular area\n",
    "\n",
    "point 5 - People with scientific backgrounds (physics, maths, chemistry..) make better data scientists that those with social science skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Ideas -\n",
    "\n",
    "1. Limit Order resting times and fill rates accross continuous trading. This project will look into the time it takes for a limit order to execute in the market and fill rates of Fill and Kill market orders.\n",
    "    -Data : ASX ITCH Data\n",
    "    -The results will take the form of percentage statistics accross multiple days and/or a time series.\n",
    "    -ASX order level data over a few sample days.\n",
    "    -This will require a fairly complex algorithm and manipulation of GB level data sets. Basic maths & stats will be required\n",
    "    \n",
    "2. Execution performance of Sweep orders vs Limit orders\n",
    "    -analysis of fill rates for FAK market orders vs limit orders.\n",
    "    -the results will include descriptive statistics on times taken for execution, and order quantity filled.\n",
    "    -ASX order level data over a few sample days\n",
    "    -This will require a fairly complex algorithm and manipulation of GB level data sets. Basic maths & stats will be required\n",
    "    \n",
    "3. Create a predictive model of ASX on-market share and test the strength of potential drivers\n",
    "    -ASX & IRESS trade/broker level data\n",
    "    -result will be in the form of a regression model where the coefficients for each variable are deduced and tested for significance\n",
    "    -this will require regression and statistical tests for significance.\n",
    "\n",
    "4. Execution quality of block trades in auctions vs off market block trades.\n",
    "    -Analysis of large sized trades conducted on market vs off market. Looking to determine the differrences in quality of off market vs on market block trades. Both qualitative and quantitative descriptive results.\n",
    "    -ASX trade level data will be required.\n",
    "    -an algorithm will be required to analyse the trade level data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Four - Movie Lens Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for each occupation in 'users', count the number of occurrences\n",
    "\n",
    "# for each occupation, calculate the mean age\n",
    "\n",
    "# for each occupation, calculate the minimum and maximum ages\n",
    "\n",
    "# for each combination of occupation and gender, calculate the mean age\n",
    "\n",
    "# randomly sample a DataFrame\n",
    "\n",
    "# detect duplicate users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pandas\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each occupation in 'users', count the number of occurrences\n",
    "\n",
    "#(done). I manually moved the u.user file into the homework folder in order to read it. not sure \n",
    "#if this is the correct way to do it\n",
    "\n",
    "#pd.read_table('https://raw.githubusercontent.com/alasdaird/SYD_DAT_6/master/labs/Week%201/u.user')\n",
    "\n",
    "user_cols = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\n",
    "users = pd.read_table('u.user', sep='|', header=None, names=user_cols, index_col='user_id', dtype={'zip_code':str})\n",
    "#users\n",
    "#users.occupation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each occupation, calculate the mean age\n",
    "# done\n",
    "\n",
    "#users.groupby('occupation').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each occupation, calculate the minimum and maximum ages\n",
    "\n",
    "#done\n",
    "\n",
    "#users.groupby('occupation')['age'].max()\n",
    "#users.groupby('occupation')['age'].min()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each combination of occupation and gender, calculate the mean age\n",
    "\n",
    "#done\n",
    "\n",
    "#users.groupby(['occupation', 'gender'])['age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# randomly sample a DataFrame\n",
    "\n",
    "#Done\n",
    "\n",
    "#randomly sampled  20 items from the data\n",
    "\n",
    "#users.sample(20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# detect duplicate users\n",
    "\n",
    "#done\n",
    "\n",
    "#detects duplicate user_ids\n",
    "\n",
    "#users.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lab from Class\n",
    "\n",
    "# filter DataFrame to only include European countries\n",
    "#(done)\n",
    "\n",
    "drinks = pd.read_table('drinks.csv', sep = \",\")\n",
    "#drinks\n",
    "#drinks[drinks.continent.isin(['EU'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lab from Class\n",
    "\n",
    "# filter DataFrame to only include European countries with wine_servings > 300\n",
    "\n",
    "#(done)\n",
    "#WineServing = drinks.wine_servings > 300\n",
    "#drinks[WineServing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lab from Class\n",
    "\n",
    "# calculate the average 'beer_servings' for all of Europe\n",
    "\n",
    "#(done)\n",
    "#EuropeBeer = drinks[(drinks.continent =='EU')]\n",
    "#EuropeBeer.beer_servings.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lab from Class\n",
    "\n",
    "# determine which 10 countries have the highest total_litres_of_pure_alcohol\n",
    "\n",
    "#(done)\n",
    "#drinks.sort_values(by='total_litres_of_pure_alcohol', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lab from Class\n",
    "\n",
    "# rename the column 'beer_servings' to 'beer'\n",
    "#(done)\n",
    "\n",
    "#cols_change = ['country', 'beer', 'spirit_servings', 'wine_servings', 'total_litres_of_pure_alcohol' , 'continent']\n",
    "#cols_change\n",
    "#drinks1 = pd.read_table('drinks.csv', sep=',')\n",
    "#drinks1.rename(columns = {'beer_servings' : 'beer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lab from Class\n",
    "\n",
    "# add a new column as a function of existing columns, total_servings = beer + wine + spirits\n",
    "#(done)\n",
    "\n",
    "#drinks2 = pd.read_table('drinks.csv', sep=',')\n",
    "#drinks2['total_servings'] = drinks2.sum(axis=1)\n",
    "#drinks3 = drinks2\n",
    "#drinks3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lab from Class\n",
    "\n",
    "# remove the column you just added\n",
    "#(done)\n",
    "\n",
    "#drinks3\n",
    "#drinks4 = drinks3.drop('total_servings',1)\n",
    "#drinks4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
