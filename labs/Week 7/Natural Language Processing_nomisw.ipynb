{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science with Python \n",
    "## Natural Language Processing (NLP)\n",
    "\n",
    "This notebook contains exercises for getting started with Natual Language Processing in Python. The main topics we will cover in this class are:\n",
    "\n",
    "Introduction\n",
    "1. newsgroups dataset\n",
    "- Bag of words prediciton model\n",
    "\n",
    "Advanced Language Processing with NLTK \n",
    "1. Tokenizing\n",
    "- Stemming\n",
    "- Speech Tagging\n",
    "- Named Entity Recognition\n",
    "- Term Frequency - Inverse Document Frequency\n",
    "- Latent Dirichlet Allocation\n",
    "- Regex\n",
    "\n",
    "Other:\n",
    "- Analysing data with the Alchemy API - http://www.alchemyapi.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Prediction\n",
    "\n",
    "We will use the [20 Newsgroup dataset](http://qwone.com/~jason/20Newsgroups/), which is provided by Scikit-Learn. This is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "We will restrict the analysis to 4 groups and will attempt to classify them starting from the corresponding text.\n",
    "\n",
    "This is a typical example of text classification, where a data scientist's task is to train a model that can partition text in pre-defined categories. Other examples include sentiment analysis and topic assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data inspection\n",
    "\n",
    "We have downloaded a few newsgroup categories and removed headers, footers and quotes.\n",
    "\n",
    "Let's inspect them.\n",
    "\n",
    "1. What data taype is `data_train`\n",
    "> sklearn.datasets.base.Bunch\n",
    "- Is it like a list? Or like a Dictionary? or what?\n",
    "> Dict\n",
    "- How many data points does it contain?\n",
    "- Inspect the first data point, what does it look like?\n",
    "> A blurb of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"\\n >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \\n\\nMB>                                                             So the\\nMB> 1970 figure seems unlikely to actually be anything but a perijove.\\n\\nJG>Sorry, _perijoves_...I'm not used to talking this language.\\n\\nCouldn't we just say periapsis or apoapsis?\\n\\n \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag of Words model\n",
    "\n",
    "Let's train a model using a simple count vectorizer\n",
    "\n",
    "[Text Feature Extraction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "1. Initialize a standard CountVectorizer and fit the training data (tokenization and occurrence counting)\n",
    "- how big is the feature dictionary\n",
    "- repeat eliminating english stop words\n",
    "- is the dictionary smaller?\n",
    "- transform the training data using the trained vectorizer\n",
    "- what are the 20 words that are most common in the whole corpus?\n",
    "- what are the 20 most common words in each of the 4 classes?\n",
    "- evaluate the performance of a Lotistic Regression on the features extracted by the CountVectorizer\n",
    "    - you will have to transform the test_set too. Be carefule to use the trained vectorizer, without re-fitting it\n",
    "- try the following 3 modification:\n",
    "    - restrict the max_features\n",
    "    - change max_df and min_df\n",
    "    - use a fixed vocabulary of size 80 combining the 20 most common words per group found earlier\n",
    "- for each of the above print a confusion matrix and investigate what gets mixed\n",
    "> Anwer: not surprisingly if we reduce the feature space we lose accuracy\n",
    "- print out the number of features for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec = CountVectorizer()\n",
    "cvec.fit(data_train['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26879"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26577"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec = CountVectorizer(stop_words='english')\n",
    "cvec.fit(data_train['data'])\n",
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(cvec.transform(data_train['data']).todense(),\n",
    "                       columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = data_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "space       1061\n",
       "people       793\n",
       "god          745\n",
       "don          730\n",
       "like         682\n",
       "just         675\n",
       "does         600\n",
       "know         592\n",
       "think        584\n",
       "time         546\n",
       "image        534\n",
       "edu          501\n",
       "use          468\n",
       "good         449\n",
       "data         444\n",
       "nasa         419\n",
       "graphics     414\n",
       "jesus        411\n",
       "say          409\n",
       "way          387\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = X_train.sum(axis=0)\n",
    "word_counts.sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = data_train['target_names']\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism most common words\n",
      "god         405\n",
      "people      330\n",
      "don         262\n",
      "think       215\n",
      "just        209\n",
      "does        207\n",
      "atheism     199\n",
      "say         174\n",
      "believe     163\n",
      "like        162\n",
      "atheists    162\n",
      "religion    156\n",
      "jesus       155\n",
      "know        154\n",
      "argument    148\n",
      "time        135\n",
      "said        131\n",
      "true        131\n",
      "bible       121\n",
      "way         120\n",
      "dtype: int64\n",
      "\n",
      "comp.graphics most common words\n",
      "image        484\n",
      "graphics     410\n",
      "edu          297\n",
      "jpeg         267\n",
      "file         265\n",
      "use          225\n",
      "data         219\n",
      "files        217\n",
      "images       212\n",
      "software     212\n",
      "program      199\n",
      "ftp          189\n",
      "available    185\n",
      "format       178\n",
      "color        174\n",
      "like         167\n",
      "know         165\n",
      "pub          161\n",
      "gif          160\n",
      "does         157\n",
      "dtype: int64\n",
      "\n",
      "sci.space most common words\n",
      "space        989\n",
      "nasa         374\n",
      "launch       267\n",
      "earth        222\n",
      "like         222\n",
      "data         216\n",
      "orbit        201\n",
      "time         197\n",
      "shuttle      192\n",
      "just         189\n",
      "satellite    187\n",
      "lunar        182\n",
      "moon         168\n",
      "new          158\n",
      "program      156\n",
      "don          151\n",
      "year         146\n",
      "people       142\n",
      "mission      141\n",
      "use          134\n",
      "dtype: int64\n",
      "\n",
      "talk.religion.misc most common words\n",
      "god          329\n",
      "people       267\n",
      "jesus        256\n",
      "don          162\n",
      "bible        160\n",
      "just         159\n",
      "christian    151\n",
      "think        151\n",
      "say          149\n",
      "know         149\n",
      "does         147\n",
      "did          132\n",
      "like         131\n",
      "good         131\n",
      "life         118\n",
      "way          118\n",
      "believe      117\n",
      "said         103\n",
      "point        101\n",
      "time          99\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "common_words = []\n",
    "for i in xrange(4):\n",
    "    word_count = X_train[y_train==i].sum(axis=0)\n",
    "    print names[i], \"most common words\"\n",
    "    cw = word_count.sort_values(ascending = False).head(20)\n",
    "#     cw.to_csv('../../../5.2-lesson/assets/datasets/'+names[i]+'_most_common_words.csv')\n",
    "    print cw\n",
    "    common_words.extend(cw.index)\n",
    "    print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(cvec.transform(data_test['data']).todense(),\n",
    "                      columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(cvec.transform(data_test['data']).todense(),\n",
    "                      columns=cvec.get_feature_names())\n",
    "y_test = data_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.745750184774575"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docm(y_true, y_pred, labels=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if labels is not None:\n",
    "        cols = ['p_'+c for c in labels]\n",
    "        df = pd.DataFrame(cm, index=labels, columns=cols)\n",
    "    else:\n",
    "        cols = ['p_'+str(i) for i in xrange(len(cm))]\n",
    "        df = pd.DataFrame(cm, columns=cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69623059867\n",
      "Number of features: 1000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(CountVectorizer(stop_words='english',\n",
    "                                      max_features=1000),\n",
    "                      LogisticRegression(),\n",
    "                      )\n",
    "model.fit(data_train['data'], y_train)\n",
    "y_pred = model.predict(data_test['data'])\n",
    "print accuracy_score(y_test, y_pred)\n",
    "docm(y_test, y_pred, names)\n",
    "print \"Number of features:\", len(model.steps[0][1].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_alt.atheism</th>\n",
       "      <th>p_comp.graphics</th>\n",
       "      <th>p_sci.space</th>\n",
       "      <th>p_talk.religion.misc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alt.atheism</th>\n",
       "      <td>160</td>\n",
       "      <td>67</td>\n",
       "      <td>33</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.graphics</th>\n",
       "      <td>25</td>\n",
       "      <td>314</td>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.space</th>\n",
       "      <td>37</td>\n",
       "      <td>88</td>\n",
       "      <td>247</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.religion.misc</th>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>15</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    p_alt.atheism  p_comp.graphics  p_sci.space  \\\n",
       "alt.atheism                   160               67           33   \n",
       "comp.graphics                  25              314           44   \n",
       "sci.space                      37               88          247   \n",
       "talk.religion.misc             90               70           15   \n",
       "\n",
       "                    p_talk.religion.misc  \n",
       "alt.atheism                           59  \n",
       "comp.graphics                          6  \n",
       "sci.space                             22  \n",
       "talk.religion.misc                    76  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docm(y_test,y_pred,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.650406504065\n",
      "Number of features: 258\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(CountVectorizer(stop_words='english',\n",
    "                                      max_features=1000,\n",
    "                                      min_df=0.03),\n",
    "                      LogisticRegression(),\n",
    "                      )\n",
    "model.fit(data_train['data'], y_train)\n",
    "y_pred = model.predict(data_test['data'])\n",
    "print accuracy_score(y_test, y_pred)\n",
    "docm(y_test, y_pred, names)\n",
    "print \"Number of features:\", len(model.steps[0][1].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.589061345159\n",
      "Number of features: 54\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(CountVectorizer(stop_words='english',\n",
    "                                      vocabulary=set(common_words)),\n",
    "                      LogisticRegression(),\n",
    "                      )\n",
    "model.fit(data_train['data'], y_train)\n",
    "y_pred = model.predict(data_test['data'])\n",
    "print accuracy_score(y_test, y_pred)\n",
    "docm(y_test, y_pred, names)\n",
    "print \"Number of features:\", len(model.steps[0][1].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Natural Language Processing with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following step, see the new window that will pop out elsewhere. The whole NLTK package is huge. Too big for all of you to download in class at the same time. Instead, select to install just the main packages and see of the lab runs. If not, use the downloader to get the additional missing packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# find pop-up window for downloader tool\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "What:  Separate text into units such as sentences or words\n",
    "\n",
    "Why:   Gives structure to previously unstructured text\n",
    "\n",
    "Notes: Relatively easy with English language text, not easy with some languages\n",
    "\n",
    "\n",
    "\"corpus\" = collection of documents\n",
    "\n",
    "\"corpora\" = plural form of corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Data science is an interdisciplinary field about processes and systems to extract knowledge or insights from data in various forms, either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, machine learning, data mining, and predictive analytics,[3] similar to Knowledge Discovery in Databases (KDD).   Data science employs techniques and theories drawn from many fields within the broad areas of mathematics, statistics, operations research,[4] information science, and computer science, including signal processing, probability models, machine learning, statistical learning, data mining, database, data engineering, pattern recognition and learning, visualization, predictive analytics, uncertainty modeling, data warehousing, data compression, computer programming, artificial intelligence, and high performance computing. Methods that scale to big data are of particular interest in data science, although the discipline is not generally considered to be restricted to such big data, and big data technologies are often focused on organizing and preprocessing the data instead of analysis. The development of machine learning has enhanced the growth and importance of data science. Data science affects academic and applied research in many domains, including machine translation, speech recognition, robotics, search engines, digital economy, but also the biological sciences, medical informatics, health care, social sciences and the humanities. It heavily influences economics, business and finance. From the business perspective, data science is an integral part of competitive intelligence, a newly emerging field that encompasses a number of activities, such as data mining and data analysis.[5] Data scientists use their data and analytical ability to find and interpret rich data sources; manage large amounts of data despite hardware, software, and bandwidth constraints; merge data sources; ensure consistency of datasets; create visualizations to aid in understanding data; build mathematical models using the data; and present and communicate the data insights/findings. They are often expected to produce answers in days rather than months, work by exploratory analysis and rapid iteration, and to produce and present results with dashboards (displays of current values) rather than papers/reports, as statisticians normally do.[6] \"Data Scientist\" has become a popular occupation with Harvard Business Review dubbing it \"The Sexiest Job of the 21st Century\" [7] and McKinsey & Company projecting a global excess demand of 1.5 million new data scientists.[8] Universities are offering masters courses in data science.[9] Shorter private bootcamps are also offering data science certificates including student-paid programs like General Assembly to employer-paid programs like The Data Incubator.[10] The term \"data science\" (originally used interchangeably with \"datalogy\") has existed for over thirty years and was used initially as a substitute for computer science by Peter Naur in 1960. In 1974, Naur published Concise Survey of Computer Methods, which freely used the term data science in its survey of the contemporary data processing methods that are used in a wide range of applications. In 1996, members of the International Federation of Classification Societies (IFCS) met in Kobe for their biennial conference. Here, for the first time, the term data science is included in the title of the conference (\"Data Science, classification, and related methods\").[11] In November 1997, C.F. Jeff Wu gave the inaugural lecture entitled \"Statistics = Data Science?\"[12] for his appointment to the H. C. Carver Professorship at the University of Michigan.[13] In this lecture, he characterized statistical work as a trilogy of data collection, data modeling and analysis, and decision making. In his conclusion, he initiated the modern, non-computer science, usage of the term \"data science\" and advocated that statistics be renamed data science and statisticians data scientists.[12] Later, he presented his lecture entitled \"Statistics = Data Science?\" as the first of his 1998 P.C. Mahalanobis Memorial Lectures.[14] These lectures honor Prasanta Chandra Mahalanobis, an Indian scientist and statistician and founder of the Indian Statistical Institute. In 2001, William S. Cleveland introduced data science as an independent discipline, extending the field of statistics to incorporate \"advances in computing with data\" in his article \"Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics,\" which was published in Volume 69, No. 1, of the April 2001 edition of the International Statistical Review / Revue Internationale de Statistique.[15] In his report, Cleveland establishes six technical areas which he believed to encompass the field of data science: multidisciplinary investigations, models and methods for data, computing with data, pedagogy, tool evaluation, and theory. In April 2002, the International Council for Science: Committee on Data for Science and Technology (CODATA)[16] started the Data Science Journal,[17] a publication focused on issues such as the description of data systems, their publication on the internet, applications and legal issues.[18] Shortly thereafter, in January 2003, Columbia University began publishing The Journal of Data Science,[19] which provided a platform for all data workers to present their views and exchange ideas. The journal was largely devoted to the application of statistical methods and quantitative research. In 2005, The National Science Board published \"Long-lived Digital Data Collections: Enabling Research and Education in the 21st Century\" defining data scientists as \"the information and computer scientists, database and software and programmers, disciplinary experts, curators and expert annotators, librarians, archivists, and others, who are crucial to the successful management of a digital data collection\" whose primary activity is to \"conduct creative inquiry and analysis.\"[20] In 2013, the IEEE Task Force on Data Science and Advanced Analytics [21] was launched, and the first international conference: IEEE International Conference on Data Science and Advanced Analytics was launched in 2014.[22] In 2015, the International Journal on Data Science and Analytics [23] was launched by Springer to publish original work on data science and big data analytics. In 2008,[citation needed] DJ Patil and Jeff Hammerbacher used the term \"data scientist\" to define their jobs at LinkedIn and Facebook, respectively.[24] Although use of the term \"data science\" has exploded in business environments, many academics and journalists see no distinction between data science and statistics. Writing in Forbes, Gil Press argues that data science is a buzzword without a clear definition and has simply replaced \\u201cbusiness analytics\\u201d in contexts such as graduate degree programs.[25] In the question-and-answer section of his keynote address at the Joint Statistical Meetings of American Statistical Association, noted applied statistician Nate Silver said, \\u201cI think data-scientist is a sexed up term for a statistician....Statistics is a branch of science. Data scientist is slightly redundant in some way and people shouldn\\u2019t berate the term statistician.\\u201d[26] In the 2010-2011 time frame, data science software reached an inflection point where open source software started supplanting proprietary software.[27] The use of open source software enables modifying and extending the software, and it allows sharing of the resulting algorithms.[28][29][30] '"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get(\"http://en.wikipedia.org/wiki/Data_science\")\n",
    "b = BeautifulSoup(r.text, \"lxml\")\n",
    "paragraphs = b.find(\"body\").findAll(\"p\")\n",
    "text = \"\"\n",
    "for paragraph in paragraphs:\n",
    "    text += paragraph.text + \" \"\n",
    "# Data Science corpus\n",
    "text[:500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Data science is an interdisciplinary field about processes and systems to extract knowledge or insights from data in various forms, either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, machine learning, data mining, and predictive analytics,[3] similar to Knowledge Discovery in Databases (KDD).',\n",
       " u'Data science employs techniques and theories drawn from many fields within the broad areas of mathematics, statistics, operations research,[4] information science, and computer science, including signal processing, probability models, machine learning, statistical learning, data mining, database, data engineering, pattern recognition and learning, visualization, predictive analytics, uncertainty modeling, data warehousing, data compression, computer programming, artificial intelligence, and high performance computing.',\n",
       " u'Methods that scale to big data are of particular interest in data science, although the discipline is not generally considered to be restricted to such big data, and big data technologies are often focused on organizing and preprocessing the data instead of analysis.',\n",
       " u'The development of machine learning has enhanced the growth and importance of data science.',\n",
       " u'Data science affects academic and applied research in many domains, including machine translation, speech recognition, robotics, search engines, digital economy, but also the biological sciences, medical informatics, health care, social sciences and the humanities.',\n",
       " u'It heavily influences economics, business and finance.',\n",
       " u'From the business perspective, data science is an integral part of competitive intelligence, a newly emerging field that encompasses a number of activities, such as data mining and data analysis.',\n",
       " u'[5] Data scientists use their data and analytical ability to find and interpret rich data sources; manage large amounts of data despite hardware, software, and bandwidth constraints; merge data sources; ensure consistency of datasets; create visualizations to aid in understanding data; build mathematical models using the data; and present and communicate the data insights/findings.',\n",
       " u'They are often expected to produce answers in days rather than months, work by exploratory analysis and rapid iteration, and to produce and present results with dashboards (displays of current values) rather than papers/reports, as statisticians normally do.',\n",
       " u'[6] \"Data Scientist\" has become a popular occupation with Harvard Business Review dubbing it \"The Sexiest Job of the 21st Century\" [7] and McKinsey & Company projecting a global excess demand of 1.5 million new data scientists.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize into sentences\n",
    "sentences = [sent for sent in nltk.sent_tokenize(text)]\n",
    "sentences[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Data',\n",
       " u'science',\n",
       " u'is',\n",
       " u'an',\n",
       " u'interdisciplinary',\n",
       " u'field',\n",
       " u'about',\n",
       " u'processes',\n",
       " u'and',\n",
       " u'systems',\n",
       " u'to',\n",
       " u'extract',\n",
       " u'knowledge',\n",
       " u'or',\n",
       " u'insights',\n",
       " u'from',\n",
       " u'data',\n",
       " u'in',\n",
       " u'various',\n",
       " u'forms',\n",
       " u',',\n",
       " u'either',\n",
       " u'structured',\n",
       " u'or',\n",
       " u'unstructured',\n",
       " u',',\n",
       " u'[',\n",
       " u'1',\n",
       " u']',\n",
       " u'[',\n",
       " u'2',\n",
       " u']',\n",
       " u'which',\n",
       " u'is',\n",
       " u'a',\n",
       " u'continuation',\n",
       " u'of',\n",
       " u'some',\n",
       " u'of',\n",
       " u'the',\n",
       " u'data',\n",
       " u'analysis',\n",
       " u'fields',\n",
       " u'such',\n",
       " u'as',\n",
       " u'statistics',\n",
       " u',',\n",
       " u'machine',\n",
       " u'learning',\n",
       " u',',\n",
       " u'data',\n",
       " u'mining',\n",
       " u',',\n",
       " u'and',\n",
       " u'predictive',\n",
       " u'analytics',\n",
       " u',',\n",
       " u'[',\n",
       " u'3',\n",
       " u']',\n",
       " u'similar',\n",
       " u'to',\n",
       " u'Knowledge',\n",
       " u'Discovery',\n",
       " u'in',\n",
       " u'Databases',\n",
       " u'(',\n",
       " u'KDD',\n",
       " u')',\n",
       " u'.',\n",
       " u'Data',\n",
       " u'science',\n",
       " u'employs',\n",
       " u'techniques',\n",
       " u'and',\n",
       " u'theories',\n",
       " u'drawn',\n",
       " u'from',\n",
       " u'many',\n",
       " u'fields',\n",
       " u'within',\n",
       " u'the',\n",
       " u'broad',\n",
       " u'areas',\n",
       " u'of',\n",
       " u'mathematics',\n",
       " u',',\n",
       " u'statistics',\n",
       " u',',\n",
       " u'operations',\n",
       " u'research',\n",
       " u',',\n",
       " u'[',\n",
       " u'4',\n",
       " u']',\n",
       " u'information',\n",
       " u'science',\n",
       " u',',\n",
       " u'and',\n",
       " u'computer']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize into words\n",
    "tokens = [word for word in nltk.word_tokenize(text)]\n",
    "tokens[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Data',\n",
       " u'science',\n",
       " u'is',\n",
       " u'an',\n",
       " u'interdisciplinary',\n",
       " u'field',\n",
       " u'about',\n",
       " u'processes',\n",
       " u'and',\n",
       " u'systems',\n",
       " u'to',\n",
       " u'extract',\n",
       " u'knowledge',\n",
       " u'or',\n",
       " u'insights',\n",
       " u'from',\n",
       " u'data',\n",
       " u'in',\n",
       " u'various',\n",
       " u'forms',\n",
       " u'either',\n",
       " u'structured',\n",
       " u'or',\n",
       " u'unstructured',\n",
       " u'which',\n",
       " u'is',\n",
       " u'a',\n",
       " u'continuation',\n",
       " u'of',\n",
       " u'some',\n",
       " u'of',\n",
       " u'the',\n",
       " u'data',\n",
       " u'analysis',\n",
       " u'fields',\n",
       " u'such',\n",
       " u'as',\n",
       " u'statistics',\n",
       " u'machine',\n",
       " u'learning',\n",
       " u'data',\n",
       " u'mining',\n",
       " u'and',\n",
       " u'predictive',\n",
       " u'analytics',\n",
       " u'similar',\n",
       " u'to',\n",
       " u'Knowledge',\n",
       " u'Discovery',\n",
       " u'in',\n",
       " u'Databases',\n",
       " u'KDD',\n",
       " u'Data',\n",
       " u'science',\n",
       " u'employs',\n",
       " u'techniques',\n",
       " u'and',\n",
       " u'theories',\n",
       " u'drawn',\n",
       " u'from',\n",
       " u'many',\n",
       " u'fields',\n",
       " u'within',\n",
       " u'the',\n",
       " u'broad',\n",
       " u'areas',\n",
       " u'of',\n",
       " u'mathematics',\n",
       " u'statistics',\n",
       " u'operations',\n",
       " u'research',\n",
       " u'information',\n",
       " u'science',\n",
       " u'and',\n",
       " u'computer',\n",
       " u'science',\n",
       " u'including',\n",
       " u'signal',\n",
       " u'processing',\n",
       " u'probability',\n",
       " u'models',\n",
       " u'machine',\n",
       " u'learning',\n",
       " u'statistical',\n",
       " u'learning',\n",
       " u'data',\n",
       " u'mining',\n",
       " u'database',\n",
       " u'data',\n",
       " u'engineering',\n",
       " u'pattern',\n",
       " u'recognition',\n",
       " u'and',\n",
       " u'learning',\n",
       " u'visualization',\n",
       " u'predictive',\n",
       " u'analytics',\n",
       " u'uncertainty',\n",
       " u'modeling',\n",
       " u'data']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only keep tokens that start with a letter (using regular expressions)\n",
    "import re\n",
    "clean_tokens = [token for token in tokens if re.search('^[a-zA-Z]+', token)]\n",
    "clean_tokens[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'all', 1),\n",
       " (u'founder', 1),\n",
       " (u'global', 1),\n",
       " (u'results', 1),\n",
       " (u'Gil', 1),\n",
       " (u'issues', 2),\n",
       " (u'devoted', 1),\n",
       " (u'whose', 1),\n",
       " (u'Columbia', 1),\n",
       " (u'Business', 1),\n",
       " (u'description', 1),\n",
       " (u'to', 19),\n",
       " (u'started', 2),\n",
       " (u'activities', 1),\n",
       " (u'appointment', 1),\n",
       " (u'unstructured', 1),\n",
       " (u'presented', 1),\n",
       " (u'large', 1),\n",
       " (u'science', 25),\n",
       " (u'biological', 1),\n",
       " (u'noted', 1),\n",
       " (u'application', 1),\n",
       " (u'perspective', 1),\n",
       " (u'Hammerbacher', 1),\n",
       " (u'substitute', 1),\n",
       " (u'section', 1),\n",
       " (u'bootcamps', 1),\n",
       " (u'current', 1),\n",
       " (u'student-paid', 1),\n",
       " (u'establishes', 1),\n",
       " (u'scientists', 5),\n",
       " (u'certificates', 1),\n",
       " (u'new', 1),\n",
       " (u'journal', 1),\n",
       " (u'degree', 1),\n",
       " (u'exchange', 1),\n",
       " (u'respectively', 1),\n",
       " (u'answers', 1),\n",
       " (u'understanding', 1),\n",
       " (u'met', 1),\n",
       " (u'focused', 2),\n",
       " (u'excess', 1),\n",
       " (u'interpret', 1),\n",
       " (u'search', 1),\n",
       " (u'Long-lived', 1),\n",
       " (u'thirty', 1),\n",
       " (u'warehousing', 1),\n",
       " (u'technical', 1),\n",
       " (u'economics', 1),\n",
       " (u'allows', 1),\n",
       " (u'survey', 1),\n",
       " (u'published', 3),\n",
       " (u'domains', 1),\n",
       " (u'Technology', 1),\n",
       " (u'honor', 1),\n",
       " (u'inflection', 1),\n",
       " (u'classification', 1),\n",
       " (u'manage', 1),\n",
       " (u'private', 1),\n",
       " (u'modifying', 1),\n",
       " (u'S.', 1),\n",
       " (u'use', 3),\n",
       " (u'from', 2),\n",
       " (u'Joint', 1),\n",
       " (u'employer-paid', 1),\n",
       " (u'sharing', 1),\n",
       " (u'journalists', 1),\n",
       " (u'initiated', 1),\n",
       " (u'These', 1),\n",
       " (u'Professorship', 1),\n",
       " (u'American', 1),\n",
       " (u'particular', 1),\n",
       " (u'insights', 1),\n",
       " (u'Concise', 1),\n",
       " (u'this', 1),\n",
       " (u'Expanding', 1),\n",
       " (u'originally', 1),\n",
       " (u'work', 3),\n",
       " (u'theories', 1),\n",
       " (u'values', 1),\n",
       " (u'believed', 1),\n",
       " (u'making', 1),\n",
       " (u'pedagogy', 1),\n",
       " (u'address', 1),\n",
       " (u'Conference', 1),\n",
       " (u'proprietary', 1),\n",
       " (u'high', 1),\n",
       " (u'Forbes', 1),\n",
       " (u'employs', 1),\n",
       " (u'Digital', 1),\n",
       " (u'information', 2),\n",
       " (u'Federation', 1),\n",
       " (u'rather', 2),\n",
       " (u'Journal', 3),\n",
       " (u'six', 1),\n",
       " (u'statisticians', 2),\n",
       " (u'inaugural', 1),\n",
       " (u'machine', 4),\n",
       " (u'influences', 1),\n",
       " (u'instead', 1),\n",
       " (u'buzzword', 1),\n",
       " (u'economy', 1),\n",
       " (u'Committee', 1),\n",
       " (u'Technical', 1),\n",
       " (u'collection', 2),\n",
       " (u'CODATA', 1),\n",
       " (u'applications', 2),\n",
       " (u'produce', 2),\n",
       " (u'such', 5),\n",
       " (u'data', 51),\n",
       " (u'a', 16),\n",
       " (u'Incubator', 1),\n",
       " (u'advocated', 1),\n",
       " (u'redundant', 1),\n",
       " (u'algorithms', 1),\n",
       " (u'operations', 1),\n",
       " (u'over', 1),\n",
       " (u'years', 1),\n",
       " (u'scientist', 3),\n",
       " (u'including', 3),\n",
       " (u'statistical', 3),\n",
       " (u'its', 1),\n",
       " (u'Lectures', 1),\n",
       " (u'multidisciplinary', 1),\n",
       " (u'encompasses', 1),\n",
       " (u'Here', 1),\n",
       " (u'forms', 1),\n",
       " (u'platform', 1),\n",
       " (u'systems', 2),\n",
       " (u'inquiry', 1),\n",
       " (u'freely', 1),\n",
       " (u'defining', 1),\n",
       " (u'thereafter', 1),\n",
       " (u'Task', 1),\n",
       " (u'finance', 1),\n",
       " (u'interdisciplinary', 1),\n",
       " (u'views', 1),\n",
       " (u'primary', 1),\n",
       " (u'academics', 1),\n",
       " (u'not', 1),\n",
       " (u'P.C', 1),\n",
       " (u'masters', 1),\n",
       " (u'Meetings', 1),\n",
       " (u'merge', 1),\n",
       " (u'Council', 1),\n",
       " (u'characterized', 1),\n",
       " (u'constraints', 1),\n",
       " (u'successful', 1),\n",
       " (u'Statistics', 3),\n",
       " (u'From', 1),\n",
       " (u'enables', 1),\n",
       " (u'related', 1),\n",
       " (u'Shorter', 1),\n",
       " (u'non-computer', 1),\n",
       " (u'extract', 1),\n",
       " (u'Discovery', 1),\n",
       " (u'restricted', 1),\n",
       " (u'Michigan', 1),\n",
       " (u'crucial', 1),\n",
       " (u'publish', 1),\n",
       " (u'research', 3),\n",
       " (u'William', 1),\n",
       " (u'Job', 1),\n",
       " (u'health', 1),\n",
       " (u'internet', 1),\n",
       " (u'lecture', 3),\n",
       " (u'newly', 1),\n",
       " (u'evaluation', 1),\n",
       " (u'Shortly', 1),\n",
       " (u'theory', 1),\n",
       " (u'University', 2),\n",
       " (u'million', 1),\n",
       " (u'members', 1),\n",
       " (u'care', 1),\n",
       " (u'definition', 1),\n",
       " (u'National', 1),\n",
       " (u'programming', 1),\n",
       " (u'days', 1),\n",
       " (u'preprocessing', 1),\n",
       " (u'think', 1),\n",
       " (u'first', 3),\n",
       " (u'Universities', 1),\n",
       " (u'probability', 1),\n",
       " (u'McKinsey', 1),\n",
       " (u'Review', 2),\n",
       " (u'number', 1),\n",
       " (u'Indian', 2),\n",
       " (u'shouldn\\u2019t', 1),\n",
       " (u'reached', 1),\n",
       " (u'open', 2),\n",
       " (u'management', 1),\n",
       " (u'publication', 2),\n",
       " (u'needed', 1),\n",
       " (u'structured', 1),\n",
       " (u'their', 5),\n",
       " (u'Press', 1),\n",
       " (u'Data', 18),\n",
       " (u'Association', 1),\n",
       " (u'mining', 3),\n",
       " (u'insights/findings', 1),\n",
       " (u'that', 5),\n",
       " (u'exploded', 1),\n",
       " (u'tool', 1),\n",
       " (u'Mahalanobis', 2),\n",
       " (u'part', 1),\n",
       " (u'translation', 1),\n",
       " (u'than', 2),\n",
       " (u'wide', 1),\n",
       " (u'quantitative', 1),\n",
       " (u'edition', 1),\n",
       " (u'question-and-answer', 1),\n",
       " (u'database', 2),\n",
       " (u'iteration', 1),\n",
       " (u'Analytics', 3),\n",
       " (u'Peter', 1),\n",
       " (u'supplanting', 1),\n",
       " (u'displays', 1),\n",
       " (u'and', 56),\n",
       " (u'DJ', 1),\n",
       " (u'largely', 1),\n",
       " (u'modern', 1),\n",
       " (u'Field', 1),\n",
       " (u'exploratory', 1),\n",
       " (u'ideas', 1),\n",
       " (u'also', 2),\n",
       " (u'build', 1),\n",
       " (u'They', 1),\n",
       " (u'January', 1),\n",
       " (u'performance', 1),\n",
       " (u'techniques', 1),\n",
       " (u'lectures', 1),\n",
       " (u'investigations', 1),\n",
       " (u'programmers', 1),\n",
       " (u'consistency', 1),\n",
       " (u'archivists', 1),\n",
       " (u'extending', 2),\n",
       " (u'Databases', 1),\n",
       " (u'The', 8),\n",
       " (u'Kobe', 1),\n",
       " (u'considered', 1),\n",
       " (u'medical', 1),\n",
       " (u'normally', 1),\n",
       " (u'incorporate', 1),\n",
       " (u'emerging', 1),\n",
       " (u'contemporary', 1),\n",
       " (u'Research', 1),\n",
       " (u'data-scientist', 1),\n",
       " (u'find', 1),\n",
       " (u'occupation', 1),\n",
       " (u'knowledge', 1),\n",
       " (u'title', 1),\n",
       " (u'papers/reports', 1),\n",
       " (u'Wu', 1),\n",
       " (u'rich', 1),\n",
       " (u'Areas', 1),\n",
       " (u'do', 1),\n",
       " (u'his', 7),\n",
       " (u'de', 1),\n",
       " (u'Enabling', 1),\n",
       " (u'despite', 1),\n",
       " (u'report', 1),\n",
       " (u'argues', 1),\n",
       " (u'areas', 2),\n",
       " (u'processes', 1),\n",
       " (u'Methods', 2),\n",
       " (u'fields', 2),\n",
       " (u'Century', 2),\n",
       " (u'activity', 1),\n",
       " (u'analytical', 1),\n",
       " (u'Education', 1),\n",
       " (u'where', 1),\n",
       " (u'conduct', 1),\n",
       " (u'predictive', 2),\n",
       " (u'intelligence', 2),\n",
       " (u'frame', 1),\n",
       " (u'analytics', 3),\n",
       " (u'see', 1),\n",
       " (u'computer', 4),\n",
       " (u'are', 7),\n",
       " (u'enhanced', 1),\n",
       " (u'interchangeably', 1),\n",
       " (u'said', 1),\n",
       " (u'expert', 1),\n",
       " (u'pattern', 1),\n",
       " (u'artificial', 1),\n",
       " (u'importance', 1),\n",
       " (u'Harvard', 1),\n",
       " (u'various', 1),\n",
       " (u'between', 1),\n",
       " (u'drawn', 1),\n",
       " (u'Statistique', 1),\n",
       " (u'Advanced', 2),\n",
       " (u'keynote', 1),\n",
       " (u'jobs', 1),\n",
       " (u'ability', 1),\n",
       " (u'initially', 1),\n",
       " (u'modeling', 2),\n",
       " (u'sexed', 1),\n",
       " (u'Carver', 1),\n",
       " (u'statistician.\\u201d', 1),\n",
       " (u'many', 3),\n",
       " (u'uncertainty', 1),\n",
       " (u'Computer', 1),\n",
       " (u'Statistical', 4),\n",
       " (u'distinction', 1),\n",
       " (u'workers', 1),\n",
       " (u'point', 1),\n",
       " (u'continuation', 1),\n",
       " (u'robotics', 1),\n",
       " (u'Sexiest', 1),\n",
       " (u'environments', 1),\n",
       " (u'simply', 1),\n",
       " (u'Board', 1),\n",
       " (u'learning', 5),\n",
       " (u'article', 1),\n",
       " (u'disciplinary', 1),\n",
       " (u'conference', 3),\n",
       " (u'Assembly', 1),\n",
       " (u'C.', 1),\n",
       " (u'engines', 1),\n",
       " (u'KDD', 1),\n",
       " (u'create', 1),\n",
       " (u'trilogy', 1),\n",
       " (u'Plan', 1),\n",
       " (u'interest', 1),\n",
       " (u'expected', 1),\n",
       " (u'annotators', 1),\n",
       " (u'Chandra', 1),\n",
       " (u'demand', 1),\n",
       " (u'Action', 1),\n",
       " (u'Prasanta', 1),\n",
       " (u'present', 3),\n",
       " (u'applied', 2),\n",
       " (u'Force', 1),\n",
       " (u'Classification', 1),\n",
       " (u'contexts', 1),\n",
       " (u'as', 10),\n",
       " (u'General', 1),\n",
       " (u'replaced', 1),\n",
       " (u'Cleveland', 2),\n",
       " (u'aid', 1),\n",
       " (u'launched', 3),\n",
       " (u'IFCS', 1),\n",
       " (u'is', 10),\n",
       " (u'it', 2),\n",
       " (u'hardware', 1),\n",
       " (u'experts', 1),\n",
       " (u'in', 22),\n",
       " (u'IEEE', 2),\n",
       " (u'began', 1),\n",
       " (u'who', 1),\n",
       " (u'speech', 1),\n",
       " (u'advances', 1),\n",
       " (u'resulting', 1),\n",
       " (u'Silver', 1),\n",
       " (u'entitled', 2),\n",
       " (u'development', 1),\n",
       " (u'independent', 1),\n",
       " (u'Springer', 1),\n",
       " (u'used', 5),\n",
       " (u'curators', 1),\n",
       " (u'Institute', 1),\n",
       " (u'Naur', 2),\n",
       " (u'social', 1),\n",
       " (u'Patil', 1),\n",
       " (u'programs', 3),\n",
       " (u'no', 1),\n",
       " (u'It', 1),\n",
       " (u'analysis', 6),\n",
       " (u'berate', 1),\n",
       " (u'academic', 1),\n",
       " (u'without', 1),\n",
       " (u'In', 14),\n",
       " (u'visualizations', 1),\n",
       " (u'the', 50),\n",
       " (u'Internationale', 1),\n",
       " (u'Memorial', 1),\n",
       " (u'heavily', 1),\n",
       " (u'competitive', 1),\n",
       " (u'sources', 2),\n",
       " (u'bandwidth', 1),\n",
       " (u'Collections', 1),\n",
       " (u'discipline', 2),\n",
       " (u'visualization', 1),\n",
       " (u'rapid', 1),\n",
       " (u'sciences', 2),\n",
       " (u'Company', 1),\n",
       " (u'source', 2),\n",
       " (u'Although', 1),\n",
       " (u'usage', 1),\n",
       " (u'has', 5),\n",
       " (u'gave', 1),\n",
       " (u'which', 5),\n",
       " (u'big', 4),\n",
       " (u'term', 8),\n",
       " (u'using', 1),\n",
       " (u'like', 2),\n",
       " (u'signal', 1),\n",
       " (u'renamed', 1),\n",
       " (u'either', 1),\n",
       " (u'popular', 1),\n",
       " (u'become', 1),\n",
       " (u'mathematical', 1),\n",
       " (u'often', 2),\n",
       " (u'people', 1),\n",
       " (u'some', 2),\n",
       " (u'growth', 1),\n",
       " (u'International', 5),\n",
       " (u'organizing', 1),\n",
       " (u'recognition', 2),\n",
       " (u'provided', 1),\n",
       " (u'scale', 1),\n",
       " (u'for', 11),\n",
       " (u'courses', 1),\n",
       " (u'affects', 1),\n",
       " (u'decision', 1),\n",
       " (u'legal', 1),\n",
       " (u'creative', 1),\n",
       " (u'Facebook', 1),\n",
       " (u'be', 2),\n",
       " (u'Later', 1),\n",
       " (u'business', 3),\n",
       " (u'statistician', 3),\n",
       " (u'processing', 2),\n",
       " (u'communicate', 1),\n",
       " (u'although', 1),\n",
       " (u'informatics', 1),\n",
       " (u'by', 3),\n",
       " (u'publishing', 1),\n",
       " (u'on', 8),\n",
       " (u'about', 1),\n",
       " (u'of', 41),\n",
       " (u'months', 1),\n",
       " (u'range', 1),\n",
       " (u'Scientist', 1),\n",
       " (u'ensure', 1),\n",
       " (u'biennial', 1),\n",
       " (u'introduced', 1),\n",
       " (u'slightly', 1),\n",
       " (u'or', 2),\n",
       " (u'software', 7),\n",
       " (u'Knowledge', 1),\n",
       " (u'No', 1),\n",
       " (u'librarians', 1),\n",
       " (u'within', 1),\n",
       " (u'integral', 1),\n",
       " (u'dashboards', 1),\n",
       " (u'Writing', 1),\n",
       " (u'methods', 4),\n",
       " (u'existed', 1),\n",
       " (u'statistics', 5),\n",
       " (u'compression', 1),\n",
       " (u'Revue', 1),\n",
       " (u'way', 1),\n",
       " (u'November', 1),\n",
       " (u'analytics\\u201d', 1),\n",
       " (u'was', 6),\n",
       " (u'Societies', 1),\n",
       " (u'offering', 2),\n",
       " (u'Science', 12),\n",
       " (u'but', 1),\n",
       " (u'Survey', 1),\n",
       " (u'technologies', 1),\n",
       " (u'with', 5),\n",
       " (u'he', 4),\n",
       " (u'datasets', 1),\n",
       " (u'Jeff', 2),\n",
       " (u'up', 1),\n",
       " (u'C.F', 1),\n",
       " (u'projecting', 1),\n",
       " (u'dubbing', 1),\n",
       " (u'define', 1),\n",
       " (u'similar', 1),\n",
       " (u'clear', 1),\n",
       " (u'computing', 3),\n",
       " (u'citation', 1),\n",
       " (u'LinkedIn', 1),\n",
       " (u'an', 5),\n",
       " (u'engineering', 1),\n",
       " (u'at', 3),\n",
       " (u'encompass', 1),\n",
       " (u'Nate', 1),\n",
       " (u'humanities', 1),\n",
       " (u'amounts', 1),\n",
       " (u'generally', 1),\n",
       " (u'graduate', 1),\n",
       " (u'Volume', 1),\n",
       " (u'field', 4),\n",
       " (u'branch', 1),\n",
       " (u'digital', 2),\n",
       " (u'mathematics', 1),\n",
       " (u'datalogy', 1),\n",
       " (u'conclusion', 1),\n",
       " (u'models', 3),\n",
       " (u'time', 2),\n",
       " (u'April', 2),\n",
       " (u'H.', 1),\n",
       " (u'included', 1),\n",
       " (u'An', 1),\n",
       " (u'broad', 1),\n",
       " (u'international', 1),\n",
       " (u'original', 1),\n",
       " (u'others', 1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the tokens\n",
    "from collections import Counter\n",
    "c = Counter(clean_tokens)\n",
    "\n",
    "c.most_common(25)       # mixed case\n",
    "c.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Action', 1),\n",
       " (u'Advanced', 2),\n",
       " (u'Although', 1),\n",
       " (u'American', 1),\n",
       " (u'An', 1),\n",
       " (u'Analytics', 3),\n",
       " (u'April', 2),\n",
       " (u'Areas', 1),\n",
       " (u'Assembly', 1),\n",
       " (u'Association', 1),\n",
       " (u'Board', 1),\n",
       " (u'Business', 1),\n",
       " (u'C.', 1),\n",
       " (u'C.F', 1),\n",
       " (u'CODATA', 1),\n",
       " (u'Carver', 1),\n",
       " (u'Century', 2),\n",
       " (u'Chandra', 1),\n",
       " (u'Classification', 1),\n",
       " (u'Cleveland', 2),\n",
       " (u'Collections', 1),\n",
       " (u'Columbia', 1),\n",
       " (u'Committee', 1),\n",
       " (u'Company', 1),\n",
       " (u'Computer', 1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(c.items())[:25]  # counts similar words separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action 1\n",
      "Advanced 2\n",
      "Although 1\n",
      "American 1\n",
      "An 1\n",
      "Analytics 3\n",
      "April 2\n",
      "Areas 1\n",
      "Assembly 1\n",
      "Association 1\n",
      "Board 1\n",
      "Business 1\n",
      "C. 1\n",
      "C.F 1\n",
      "CODATA 1\n",
      "Carver 1\n",
      "Century 2\n",
      "Chandra 1\n",
      "Classification 1\n",
      "Cleveland 2\n",
      "Collections 1\n",
      "Columbia 1\n",
      "Committee 1\n",
      "Company 1\n",
      "Computer 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'all', 1),\n",
       " (u'founder', 1),\n",
       " (u'global', 1),\n",
       " (u'results', 1),\n",
       " (u'Gil', 1),\n",
       " (u'issues', 2),\n",
       " (u'devoted', 1),\n",
       " (u'whose', 1),\n",
       " (u'Columbia', 1),\n",
       " (u'Business', 1),\n",
       " (u'description', 1),\n",
       " (u'to', 19),\n",
       " (u'started', 2),\n",
       " (u'activities', 1),\n",
       " (u'appointment', 1),\n",
       " (u'unstructured', 1),\n",
       " (u'presented', 1),\n",
       " (u'large', 1),\n",
       " (u'science', 25),\n",
       " (u'biological', 1),\n",
       " (u'noted', 1),\n",
       " (u'application', 1),\n",
       " (u'perspective', 1),\n",
       " (u'Hammerbacher', 1),\n",
       " (u'substitute', 1),\n",
       " (u'section', 1),\n",
       " (u'bootcamps', 1),\n",
       " (u'current', 1),\n",
       " (u'student-paid', 1),\n",
       " (u'establishes', 1),\n",
       " (u'scientists', 5),\n",
       " (u'certificates', 1),\n",
       " (u'new', 1),\n",
       " (u'journal', 1),\n",
       " (u'degree', 1),\n",
       " (u'exchange', 1),\n",
       " (u'respectively', 1),\n",
       " (u'answers', 1),\n",
       " (u'understanding', 1),\n",
       " (u'met', 1),\n",
       " (u'focused', 2),\n",
       " (u'excess', 1),\n",
       " (u'interpret', 1),\n",
       " (u'search', 1),\n",
       " (u'Long-lived', 1),\n",
       " (u'thirty', 1),\n",
       " (u'warehousing', 1),\n",
       " (u'technical', 1),\n",
       " (u'economics', 1),\n",
       " (u'allows', 1),\n",
       " (u'survey', 1),\n",
       " (u'published', 3),\n",
       " (u'domains', 1),\n",
       " (u'Technology', 1),\n",
       " (u'honor', 1),\n",
       " (u'inflection', 1),\n",
       " (u'classification', 1),\n",
       " (u'manage', 1),\n",
       " (u'private', 1),\n",
       " (u'modifying', 1),\n",
       " (u'S.', 1),\n",
       " (u'use', 3),\n",
       " (u'from', 2),\n",
       " (u'Joint', 1),\n",
       " (u'employer-paid', 1),\n",
       " (u'sharing', 1),\n",
       " (u'journalists', 1),\n",
       " (u'initiated', 1),\n",
       " (u'These', 1),\n",
       " (u'Professorship', 1),\n",
       " (u'American', 1),\n",
       " (u'particular', 1),\n",
       " (u'insights', 1),\n",
       " (u'Concise', 1),\n",
       " (u'this', 1),\n",
       " (u'Expanding', 1),\n",
       " (u'originally', 1),\n",
       " (u'work', 3),\n",
       " (u'theories', 1),\n",
       " (u'values', 1),\n",
       " (u'believed', 1),\n",
       " (u'making', 1),\n",
       " (u'pedagogy', 1),\n",
       " (u'address', 1),\n",
       " (u'Conference', 1),\n",
       " (u'proprietary', 1),\n",
       " (u'high', 1),\n",
       " (u'Forbes', 1),\n",
       " (u'employs', 1),\n",
       " (u'Digital', 1),\n",
       " (u'information', 2),\n",
       " (u'Federation', 1),\n",
       " (u'rather', 2),\n",
       " (u'Journal', 3),\n",
       " (u'six', 1),\n",
       " (u'statisticians', 2),\n",
       " (u'inaugural', 1),\n",
       " (u'machine', 4),\n",
       " (u'influences', 1),\n",
       " (u'instead', 1),\n",
       " (u'buzzword', 1),\n",
       " (u'economy', 1),\n",
       " (u'Committee', 1),\n",
       " (u'Technical', 1),\n",
       " (u'collection', 2),\n",
       " (u'CODATA', 1),\n",
       " (u'applications', 2),\n",
       " (u'produce', 2),\n",
       " (u'such', 5),\n",
       " (u'data', 51),\n",
       " (u'a', 16),\n",
       " (u'Incubator', 1),\n",
       " (u'advocated', 1),\n",
       " (u'redundant', 1),\n",
       " (u'algorithms', 1),\n",
       " (u'operations', 1),\n",
       " (u'over', 1),\n",
       " (u'years', 1),\n",
       " (u'scientist', 3),\n",
       " (u'including', 3),\n",
       " (u'statistical', 3),\n",
       " (u'its', 1),\n",
       " (u'Lectures', 1),\n",
       " (u'multidisciplinary', 1),\n",
       " (u'encompasses', 1),\n",
       " (u'Here', 1),\n",
       " (u'forms', 1),\n",
       " (u'platform', 1),\n",
       " (u'systems', 2),\n",
       " (u'inquiry', 1),\n",
       " (u'freely', 1),\n",
       " (u'defining', 1),\n",
       " (u'thereafter', 1),\n",
       " (u'Task', 1),\n",
       " (u'finance', 1),\n",
       " (u'interdisciplinary', 1),\n",
       " (u'views', 1),\n",
       " (u'primary', 1),\n",
       " (u'academics', 1),\n",
       " (u'not', 1),\n",
       " (u'P.C', 1),\n",
       " (u'masters', 1),\n",
       " (u'Meetings', 1),\n",
       " (u'merge', 1),\n",
       " (u'Council', 1),\n",
       " (u'characterized', 1),\n",
       " (u'constraints', 1),\n",
       " (u'successful', 1),\n",
       " (u'Statistics', 3),\n",
       " (u'From', 1),\n",
       " (u'enables', 1),\n",
       " (u'related', 1),\n",
       " (u'Shorter', 1),\n",
       " (u'non-computer', 1),\n",
       " (u'extract', 1),\n",
       " (u'Discovery', 1),\n",
       " (u'restricted', 1),\n",
       " (u'Michigan', 1),\n",
       " (u'crucial', 1),\n",
       " (u'publish', 1),\n",
       " (u'research', 3),\n",
       " (u'William', 1),\n",
       " (u'Job', 1),\n",
       " (u'health', 1),\n",
       " (u'internet', 1),\n",
       " (u'lecture', 3),\n",
       " (u'newly', 1),\n",
       " (u'evaluation', 1),\n",
       " (u'Shortly', 1),\n",
       " (u'theory', 1),\n",
       " (u'University', 2),\n",
       " (u'million', 1),\n",
       " (u'members', 1),\n",
       " (u'care', 1),\n",
       " (u'definition', 1),\n",
       " (u'National', 1),\n",
       " (u'programming', 1),\n",
       " (u'days', 1),\n",
       " (u'preprocessing', 1),\n",
       " (u'think', 1),\n",
       " (u'first', 3),\n",
       " (u'Universities', 1),\n",
       " (u'probability', 1),\n",
       " (u'McKinsey', 1),\n",
       " (u'Review', 2),\n",
       " (u'number', 1),\n",
       " (u'Indian', 2),\n",
       " (u'shouldn\\u2019t', 1),\n",
       " (u'reached', 1),\n",
       " (u'open', 2),\n",
       " (u'management', 1),\n",
       " (u'publication', 2),\n",
       " (u'needed', 1),\n",
       " (u'structured', 1),\n",
       " (u'their', 5),\n",
       " (u'Press', 1),\n",
       " (u'Data', 18),\n",
       " (u'Association', 1),\n",
       " (u'mining', 3),\n",
       " (u'insights/findings', 1),\n",
       " (u'that', 5),\n",
       " (u'exploded', 1),\n",
       " (u'tool', 1),\n",
       " (u'Mahalanobis', 2),\n",
       " (u'part', 1),\n",
       " (u'translation', 1),\n",
       " (u'than', 2),\n",
       " (u'wide', 1),\n",
       " (u'quantitative', 1),\n",
       " (u'edition', 1),\n",
       " (u'question-and-answer', 1),\n",
       " (u'database', 2),\n",
       " (u'iteration', 1),\n",
       " (u'Analytics', 3),\n",
       " (u'Peter', 1),\n",
       " (u'supplanting', 1),\n",
       " (u'displays', 1),\n",
       " (u'and', 56),\n",
       " (u'DJ', 1),\n",
       " (u'largely', 1),\n",
       " (u'modern', 1),\n",
       " (u'Field', 1),\n",
       " (u'exploratory', 1),\n",
       " (u'ideas', 1),\n",
       " (u'also', 2),\n",
       " (u'build', 1),\n",
       " (u'They', 1),\n",
       " (u'January', 1),\n",
       " (u'performance', 1),\n",
       " (u'techniques', 1),\n",
       " (u'lectures', 1),\n",
       " (u'investigations', 1),\n",
       " (u'programmers', 1),\n",
       " (u'consistency', 1),\n",
       " (u'archivists', 1),\n",
       " (u'extending', 2),\n",
       " (u'Databases', 1),\n",
       " (u'The', 8),\n",
       " (u'Kobe', 1),\n",
       " (u'considered', 1),\n",
       " (u'medical', 1),\n",
       " (u'normally', 1),\n",
       " (u'incorporate', 1),\n",
       " (u'emerging', 1),\n",
       " (u'contemporary', 1),\n",
       " (u'Research', 1),\n",
       " (u'data-scientist', 1),\n",
       " (u'find', 1),\n",
       " (u'occupation', 1),\n",
       " (u'knowledge', 1),\n",
       " (u'title', 1),\n",
       " (u'papers/reports', 1),\n",
       " (u'Wu', 1),\n",
       " (u'rich', 1),\n",
       " (u'Areas', 1),\n",
       " (u'do', 1),\n",
       " (u'his', 7),\n",
       " (u'de', 1),\n",
       " (u'Enabling', 1),\n",
       " (u'despite', 1),\n",
       " (u'report', 1),\n",
       " (u'argues', 1),\n",
       " (u'areas', 2),\n",
       " (u'processes', 1),\n",
       " (u'Methods', 2),\n",
       " (u'fields', 2),\n",
       " (u'Century', 2),\n",
       " (u'activity', 1),\n",
       " (u'analytical', 1),\n",
       " (u'Education', 1),\n",
       " (u'where', 1),\n",
       " (u'conduct', 1),\n",
       " (u'predictive', 2),\n",
       " (u'intelligence', 2),\n",
       " (u'frame', 1),\n",
       " (u'analytics', 3),\n",
       " (u'see', 1),\n",
       " (u'computer', 4),\n",
       " (u'are', 7),\n",
       " (u'enhanced', 1),\n",
       " (u'interchangeably', 1),\n",
       " (u'said', 1),\n",
       " (u'expert', 1),\n",
       " (u'pattern', 1),\n",
       " (u'artificial', 1),\n",
       " (u'importance', 1),\n",
       " (u'Harvard', 1),\n",
       " (u'various', 1),\n",
       " (u'between', 1),\n",
       " (u'drawn', 1),\n",
       " (u'Statistique', 1),\n",
       " (u'Advanced', 2),\n",
       " (u'keynote', 1),\n",
       " (u'jobs', 1),\n",
       " (u'ability', 1),\n",
       " (u'initially', 1),\n",
       " (u'modeling', 2),\n",
       " (u'sexed', 1),\n",
       " (u'Carver', 1),\n",
       " (u'statistician.\\u201d', 1),\n",
       " (u'many', 3),\n",
       " (u'uncertainty', 1),\n",
       " (u'Computer', 1),\n",
       " (u'Statistical', 4),\n",
       " (u'distinction', 1),\n",
       " (u'workers', 1),\n",
       " (u'point', 1),\n",
       " (u'continuation', 1),\n",
       " (u'robotics', 1),\n",
       " (u'Sexiest', 1),\n",
       " (u'environments', 1),\n",
       " (u'simply', 1),\n",
       " (u'Board', 1),\n",
       " (u'learning', 5),\n",
       " (u'article', 1),\n",
       " (u'disciplinary', 1),\n",
       " (u'conference', 3),\n",
       " (u'Assembly', 1),\n",
       " (u'C.', 1),\n",
       " (u'engines', 1),\n",
       " (u'KDD', 1),\n",
       " (u'create', 1),\n",
       " (u'trilogy', 1),\n",
       " (u'Plan', 1),\n",
       " (u'interest', 1),\n",
       " (u'expected', 1),\n",
       " (u'annotators', 1),\n",
       " (u'Chandra', 1),\n",
       " (u'demand', 1),\n",
       " (u'Action', 1),\n",
       " (u'Prasanta', 1),\n",
       " (u'present', 3),\n",
       " (u'applied', 2),\n",
       " (u'Force', 1),\n",
       " (u'Classification', 1),\n",
       " (u'contexts', 1),\n",
       " (u'as', 10),\n",
       " (u'General', 1),\n",
       " (u'replaced', 1),\n",
       " (u'Cleveland', 2),\n",
       " (u'aid', 1),\n",
       " (u'launched', 3),\n",
       " (u'IFCS', 1),\n",
       " (u'is', 10),\n",
       " (u'it', 2),\n",
       " (u'hardware', 1),\n",
       " (u'experts', 1),\n",
       " (u'in', 22),\n",
       " (u'IEEE', 2),\n",
       " (u'began', 1),\n",
       " (u'who', 1),\n",
       " (u'speech', 1),\n",
       " (u'advances', 1),\n",
       " (u'resulting', 1),\n",
       " (u'Silver', 1),\n",
       " (u'entitled', 2),\n",
       " (u'development', 1),\n",
       " (u'independent', 1),\n",
       " (u'Springer', 1),\n",
       " (u'used', 5),\n",
       " (u'curators', 1),\n",
       " (u'Institute', 1),\n",
       " (u'Naur', 2),\n",
       " (u'social', 1),\n",
       " (u'Patil', 1),\n",
       " (u'programs', 3),\n",
       " (u'no', 1),\n",
       " (u'It', 1),\n",
       " (u'analysis', 6),\n",
       " (u'berate', 1),\n",
       " (u'academic', 1),\n",
       " (u'without', 1),\n",
       " (u'In', 14),\n",
       " (u'visualizations', 1),\n",
       " (u'the', 50),\n",
       " (u'Internationale', 1),\n",
       " (u'Memorial', 1),\n",
       " (u'heavily', 1),\n",
       " (u'competitive', 1),\n",
       " (u'sources', 2),\n",
       " (u'bandwidth', 1),\n",
       " (u'Collections', 1),\n",
       " (u'discipline', 2),\n",
       " (u'visualization', 1),\n",
       " (u'rapid', 1),\n",
       " (u'sciences', 2),\n",
       " (u'Company', 1),\n",
       " (u'source', 2),\n",
       " (u'Although', 1),\n",
       " (u'usage', 1),\n",
       " (u'has', 5),\n",
       " (u'gave', 1),\n",
       " (u'which', 5),\n",
       " (u'big', 4),\n",
       " (u'term', 8),\n",
       " (u'using', 1),\n",
       " (u'like', 2),\n",
       " (u'signal', 1),\n",
       " (u'renamed', 1),\n",
       " (u'either', 1),\n",
       " (u'popular', 1),\n",
       " (u'become', 1),\n",
       " (u'mathematical', 1),\n",
       " (u'often', 2),\n",
       " (u'people', 1),\n",
       " (u'some', 2),\n",
       " (u'growth', 1),\n",
       " (u'International', 5),\n",
       " (u'organizing', 1),\n",
       " (u'recognition', 2),\n",
       " (u'provided', 1),\n",
       " (u'scale', 1),\n",
       " (u'for', 11),\n",
       " (u'courses', 1),\n",
       " (u'affects', 1),\n",
       " (u'decision', 1),\n",
       " (u'legal', 1),\n",
       " (u'creative', 1),\n",
       " (u'Facebook', 1),\n",
       " (u'be', 2),\n",
       " (u'Later', 1),\n",
       " (u'business', 3),\n",
       " (u'statistician', 3),\n",
       " (u'processing', 2),\n",
       " (u'communicate', 1),\n",
       " (u'although', 1),\n",
       " (u'informatics', 1),\n",
       " (u'by', 3),\n",
       " (u'publishing', 1),\n",
       " (u'on', 8),\n",
       " (u'about', 1),\n",
       " (u'of', 41),\n",
       " (u'months', 1),\n",
       " (u'range', 1),\n",
       " (u'Scientist', 1),\n",
       " (u'ensure', 1),\n",
       " (u'biennial', 1),\n",
       " (u'introduced', 1),\n",
       " (u'slightly', 1),\n",
       " (u'or', 2),\n",
       " (u'software', 7),\n",
       " (u'Knowledge', 1),\n",
       " (u'No', 1),\n",
       " (u'librarians', 1),\n",
       " (u'within', 1),\n",
       " (u'integral', 1),\n",
       " (u'dashboards', 1),\n",
       " (u'Writing', 1),\n",
       " (u'methods', 4),\n",
       " (u'existed', 1),\n",
       " (u'statistics', 5),\n",
       " (u'compression', 1),\n",
       " (u'Revue', 1),\n",
       " (u'way', 1),\n",
       " (u'November', 1),\n",
       " (u'analytics\\u201d', 1),\n",
       " (u'was', 6),\n",
       " (u'Societies', 1),\n",
       " (u'offering', 2),\n",
       " (u'Science', 12),\n",
       " (u'but', 1),\n",
       " (u'Survey', 1),\n",
       " (u'technologies', 1),\n",
       " (u'with', 5),\n",
       " (u'he', 4),\n",
       " (u'datasets', 1),\n",
       " (u'Jeff', 2),\n",
       " (u'up', 1),\n",
       " (u'C.F', 1),\n",
       " (u'projecting', 1),\n",
       " (u'dubbing', 1),\n",
       " (u'define', 1),\n",
       " (u'similar', 1),\n",
       " (u'clear', 1),\n",
       " (u'computing', 3),\n",
       " (u'citation', 1),\n",
       " (u'LinkedIn', 1),\n",
       " (u'an', 5),\n",
       " (u'engineering', 1),\n",
       " (u'at', 3),\n",
       " (u'encompass', 1),\n",
       " (u'Nate', 1),\n",
       " (u'humanities', 1),\n",
       " (u'amounts', 1),\n",
       " (u'generally', 1),\n",
       " (u'graduate', 1),\n",
       " (u'Volume', 1),\n",
       " (u'field', 4),\n",
       " (u'branch', 1),\n",
       " (u'digital', 2),\n",
       " (u'mathematics', 1),\n",
       " (u'datalogy', 1),\n",
       " (u'conclusion', 1),\n",
       " (u'models', 3),\n",
       " (u'time', 2),\n",
       " (u'April', 2),\n",
       " (u'H.', 1),\n",
       " (u'included', 1),\n",
       " (u'An', 1),\n",
       " (u'broad', 1),\n",
       " (u'international', 1),\n",
       " (u'original', 1),\n",
       " (u'others', 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for item in sorted(c.items())[:25]:\n",
    "    print item[0], item[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "abil\n",
      "about\n",
      "academ\n",
      "action\n",
      "activ\n",
      "address\n",
      "advanc\n",
      "advoc\n",
      "affect\n",
      "aid\n",
      "algorithm\n",
      "all\n",
      "allow\n",
      "also\n",
      "although\n",
      "american\n",
      "amount\n",
      "an\n",
      "analysi\n",
      "analyt\n",
      "analytics”\n",
      "and\n",
      "annot\n",
      "answer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'all', 1),\n",
       " (u'founder', 1),\n",
       " (u'global', 1),\n",
       " (u'focus', 2),\n",
       " (u'month', 1),\n",
       " (u'archivist', 1),\n",
       " (u'abil', 1),\n",
       " (u'whose', 1),\n",
       " (u'privat', 1),\n",
       " (u'educ', 1),\n",
       " (u'articl', 1),\n",
       " (u'to', 19),\n",
       " (u'program', 4),\n",
       " (u'decis', 1),\n",
       " (u'preprocess', 1),\n",
       " (u'introduc', 1),\n",
       " (u'digit', 3),\n",
       " (u'sourc', 4),\n",
       " (u'volum', 1),\n",
       " (u'affect', 1),\n",
       " (u'interchang', 1),\n",
       " (u'quantit', 1),\n",
       " (u'iter', 1),\n",
       " (u'biolog', 1),\n",
       " (u'section', 1),\n",
       " (u'contemporari', 1),\n",
       " (u'current', 1),\n",
       " (u'indian', 2),\n",
       " (u'student-paid', 1),\n",
       " (u'conduct', 1),\n",
       " (u'new', 1),\n",
       " (u'public', 2),\n",
       " (u'here', 1),\n",
       " (u'met', 1),\n",
       " (u'excess', 1),\n",
       " (u'becom', 1),\n",
       " (u'modifi', 1),\n",
       " (u'valu', 1),\n",
       " (u'search', 1),\n",
       " (u'technolog', 2),\n",
       " (u'amount', 1),\n",
       " (u'survey', 2),\n",
       " (u'thirti', 1),\n",
       " (u'social', 1),\n",
       " (u'action', 1),\n",
       " (u'prasanta', 1),\n",
       " (u'long-liv', 1),\n",
       " (u'honor', 1),\n",
       " (u'believ', 1),\n",
       " (u'curat', 1),\n",
       " (u'appli', 2),\n",
       " (u'disciplin', 2),\n",
       " (u'establish', 1),\n",
       " (u'use', 9),\n",
       " (u'from', 3),\n",
       " (u'memori', 1),\n",
       " (u'distinct', 1),\n",
       " (u'univers', 3),\n",
       " (u'process', 3),\n",
       " (u'advoc', 1),\n",
       " (u'employer-paid', 1),\n",
       " (u'peopl', 1),\n",
       " (u'inflect', 1),\n",
       " (u'relat', 1),\n",
       " (u'enhanc', 1),\n",
       " (u'visual', 2),\n",
       " (u'particular', 1),\n",
       " (u'compani', 1),\n",
       " (u'springer', 1),\n",
       " (u'this', 1),\n",
       " (u'pedagogi', 1),\n",
       " (u'work', 3),\n",
       " (u'annot', 1),\n",
       " (u'learn', 5),\n",
       " (u'meet', 1),\n",
       " (u'explod', 1),\n",
       " (u'berat', 1),\n",
       " (u'predict', 2),\n",
       " (u'in', 36),\n",
       " (u'share', 1),\n",
       " (u'high', 1),\n",
       " (u'proprietari', 1),\n",
       " (u'council', 1),\n",
       " (u'occup', 1),\n",
       " (u'cours', 1),\n",
       " (u'rather', 2),\n",
       " (u'write', 1),\n",
       " (u'six', 1),\n",
       " (u'economi', 1),\n",
       " (u'wareh', 1),\n",
       " (u'concis', 1),\n",
       " (u's.', 1),\n",
       " (u'answer', 1),\n",
       " (u'instead', 1),\n",
       " (u'buzzword', 1),\n",
       " (u'revu', 1),\n",
       " (u'de', 1),\n",
       " (u'such', 5),\n",
       " (u'redund', 1),\n",
       " (u'data', 69),\n",
       " (u'a', 16),\n",
       " (u'short', 1),\n",
       " (u'robot', 1),\n",
       " (u'produc', 2),\n",
       " (u'issu', 2),\n",
       " (u'perform', 1),\n",
       " (u'environ', 1),\n",
       " (u'incorpor', 1),\n",
       " (u'facebook', 1),\n",
       " (u'began', 1),\n",
       " (u'oper', 1),\n",
       " (u'over', 1),\n",
       " (u'insight', 1),\n",
       " (u'graduat', 1),\n",
       " (u'scientist', 9),\n",
       " (u'entitl', 2),\n",
       " (u'ieee', 2),\n",
       " (u'shorter', 1),\n",
       " (u'sexiest', 1),\n",
       " (u'platform', 1),\n",
       " (u'multidisciplinari', 1),\n",
       " (u'initi', 2),\n",
       " (u'nation', 1),\n",
       " (u'they', 1),\n",
       " (u'not', 1),\n",
       " (u'day', 1),\n",
       " (u'enabl', 2),\n",
       " (u'term', 8),\n",
       " (u'edit', 1),\n",
       " (u'perspect', 1),\n",
       " (u'professorship', 1),\n",
       " (u'chandra', 1),\n",
       " (u'ifc', 1),\n",
       " (u'januari', 1),\n",
       " (u'societi', 1),\n",
       " (u'domain', 1),\n",
       " (u'citat', 1),\n",
       " (u'intellig', 2),\n",
       " (u'replac', 1),\n",
       " (u'non-comput', 1),\n",
       " (u'continu', 1),\n",
       " (u'supplant', 1),\n",
       " (u'expect', 1),\n",
       " (u'year', 1),\n",
       " (u'extract', 1),\n",
       " (u'influenc', 1),\n",
       " (u'newli', 1),\n",
       " (u'inquiri', 1),\n",
       " (u'crucial', 1),\n",
       " (u'publish', 5),\n",
       " (u'research', 4),\n",
       " (u'health', 1),\n",
       " (u\"shouldn't\", 1),\n",
       " (u'internet', 1),\n",
       " (u'hardwar', 1),\n",
       " (u'integr', 1),\n",
       " (u'advanc', 3),\n",
       " (u'million', 1),\n",
       " (u'argu', 1),\n",
       " (u'theori', 2),\n",
       " (u'care', 1),\n",
       " (u'launch', 3),\n",
       " (u'american', 1),\n",
       " (u'think', 1),\n",
       " (u'first', 3),\n",
       " (u'origin', 2),\n",
       " (u'softwar', 7),\n",
       " (u'rang', 1),\n",
       " (u'certif', 1),\n",
       " (u'exchang', 1),\n",
       " (u'independ', 1),\n",
       " (u'number', 1),\n",
       " (u'bootcamp', 1),\n",
       " (u'restrict', 1),\n",
       " (u'kobe', 1),\n",
       " (u'open', 2),\n",
       " (u'primari', 1),\n",
       " (u'associ', 1),\n",
       " (u'system', 2),\n",
       " (u'their', 5),\n",
       " (u'master', 1),\n",
       " (u'incub', 1),\n",
       " (u'mckinsey', 1),\n",
       " (u'tool', 1),\n",
       " (u'part', 1),\n",
       " (u'technic', 2),\n",
       " (u'wide', 1),\n",
       " (u'provid', 1),\n",
       " (u'structur', 1),\n",
       " (u'project', 1),\n",
       " (u'result', 2),\n",
       " (u'and', 56),\n",
       " (u'comput', 8),\n",
       " (u'thereaft', 1),\n",
       " (u'modern', 1),\n",
       " (u'mine', 3),\n",
       " (u'need', 1),\n",
       " (u'p.c', 1),\n",
       " (u'engin', 2),\n",
       " (u'techniqu', 1),\n",
       " (u'inform', 2),\n",
       " (u'that', 5),\n",
       " (u'note', 1),\n",
       " (u'also', 2),\n",
       " (u'build', 1),\n",
       " (u'exploratori', 1),\n",
       " (u'allow', 1),\n",
       " (u'normal', 1),\n",
       " (u'who', 1),\n",
       " (u'compress', 1),\n",
       " (u'plan', 1),\n",
       " (u'librarian', 1),\n",
       " (u'model', 5),\n",
       " (u'analytics\\u201d', 1),\n",
       " (u'renam', 1),\n",
       " (u'clear', 1),\n",
       " (u'later', 1),\n",
       " (u'patil', 1),\n",
       " (u'pattern', 1),\n",
       " (u'papers/report', 1),\n",
       " (u'data-scientist', 1),\n",
       " (u'inaugur', 1),\n",
       " (u'question-and-answ', 1),\n",
       " (u'find', 1),\n",
       " (u'despit', 1),\n",
       " (u'activ', 2),\n",
       " (u'than', 2),\n",
       " (u'busi', 4),\n",
       " (u'rich', 1),\n",
       " (u'analyt', 7),\n",
       " (u'do', 1),\n",
       " (u'his', 7),\n",
       " (u'dj', 1),\n",
       " (u'michigan', 1),\n",
       " (u'financ', 1),\n",
       " (u'joint', 1),\n",
       " (u'drawn', 1),\n",
       " (u'report', 1),\n",
       " (u'unstructur', 1),\n",
       " (u'organ', 1),\n",
       " (u'naur', 2),\n",
       " (u'method', 6),\n",
       " (u'statist', 15),\n",
       " (u'where', 1),\n",
       " (u'view', 1),\n",
       " (u'trilog', 1),\n",
       " (u'frame', 1),\n",
       " (u'knowledg', 2),\n",
       " (u'sex', 1),\n",
       " (u'see', 1),\n",
       " (u'are', 7),\n",
       " (u'said', 1),\n",
       " (u'expert', 2),\n",
       " (u'databas', 3),\n",
       " (u'review', 2),\n",
       " (u'discoveri', 1),\n",
       " (u'various', 1),\n",
       " (u'between', 1),\n",
       " (u'import', 1),\n",
       " (u'extend', 2),\n",
       " (u'job', 2),\n",
       " (u'harvard', 1),\n",
       " (u'statistician.\\u201d', 1),\n",
       " (u'wu', 1),\n",
       " (u'april', 2),\n",
       " (u'committe', 1),\n",
       " (u'mani', 3),\n",
       " (u'uncertainti', 1),\n",
       " (u'simpli', 1),\n",
       " (u'jeff', 2),\n",
       " (u'address', 1),\n",
       " (u'disciplinari', 1),\n",
       " (u'codata', 1),\n",
       " (u'respect', 1),\n",
       " (u'creat', 1),\n",
       " (u'dub', 1),\n",
       " (u'interpret', 1),\n",
       " (u'interest', 1),\n",
       " (u'c.', 1),\n",
       " (u'worker', 1),\n",
       " (u'understand', 1),\n",
       " (u'demand', 1),\n",
       " (u'general', 2),\n",
       " (u'present', 4),\n",
       " (u'these', 1),\n",
       " (u'datalog', 1),\n",
       " (u'employ', 1),\n",
       " (u'defin', 2),\n",
       " (u'dashboard', 1),\n",
       " (u'aid', 1),\n",
       " (u'is', 10),\n",
       " (u'it', 4),\n",
       " (u'linkedin', 1),\n",
       " (u'centuri', 2),\n",
       " (u'scienc', 39),\n",
       " (u'develop', 1),\n",
       " (u'journal', 4),\n",
       " (u'make', 1),\n",
       " (u'medic', 1),\n",
       " (u'econom', 1),\n",
       " (u'member', 1),\n",
       " (u'speech', 1),\n",
       " (u'competit', 1),\n",
       " (u'columbia', 1),\n",
       " (u'context', 1),\n",
       " (u'freeli', 1),\n",
       " (u'expand', 1),\n",
       " (u'task', 1),\n",
       " (u'without', 1),\n",
       " (u'programm', 1),\n",
       " (u'the', 58),\n",
       " (u'academ', 2),\n",
       " (u'interdisciplinari', 1),\n",
       " (u'bandwidth', 1),\n",
       " (u'human', 1),\n",
       " (u'heavili', 1),\n",
       " (u'rapid', 1),\n",
       " (u'point', 1),\n",
       " (u'character', 1),\n",
       " (u'silver', 1),\n",
       " (u'informat', 1),\n",
       " (u'board', 1),\n",
       " (u'has', 5),\n",
       " (u'nate', 1),\n",
       " (u'gave', 1),\n",
       " (u'applic', 3),\n",
       " (u'which', 5),\n",
       " (u'big', 4),\n",
       " (u'mahalanobi', 2),\n",
       " (u'press', 1),\n",
       " (u'like', 2),\n",
       " (u'success', 1),\n",
       " (u'signal', 1),\n",
       " (u'collect', 3),\n",
       " (u'either', 1),\n",
       " (u'popular', 1),\n",
       " (u'manag', 2),\n",
       " (u'often', 2),\n",
       " (u'kdd', 1),\n",
       " (u'some', 2),\n",
       " (u'intern', 6),\n",
       " (u'growth', 1),\n",
       " (u'cleveland', 2),\n",
       " (u'peter', 1),\n",
       " (u'scale', 1),\n",
       " (u'feder', 1),\n",
       " (u'for', 11),\n",
       " (u'broad', 1),\n",
       " (u'definit', 1),\n",
       " (u'legal', 1),\n",
       " (u'recognit', 2),\n",
       " (u'substitut', 1),\n",
       " (u'mathemat', 2),\n",
       " (u'larg', 2),\n",
       " (u'journalist', 1),\n",
       " (u'machin', 4),\n",
       " (u'be', 2),\n",
       " (u'investig', 1),\n",
       " (u'hammerbach', 1),\n",
       " (u'slight', 1),\n",
       " (u'reach', 1),\n",
       " (u'statistician', 5),\n",
       " (u'usag', 1),\n",
       " (u'confer', 4),\n",
       " (u'although', 2),\n",
       " (u'statistiqu', 1),\n",
       " (u'by', 3),\n",
       " (u'on', 8),\n",
       " (u'about', 1),\n",
       " (u'keynot', 1),\n",
       " (u'constraint', 1),\n",
       " (u'of', 41),\n",
       " (u'degre', 1),\n",
       " (u'biennial', 1),\n",
       " (u'or', 2),\n",
       " (u'within', 1),\n",
       " (u'insights/find', 1),\n",
       " (u'dataset', 1),\n",
       " (u'creativ', 1),\n",
       " (u'h.', 1),\n",
       " (u'ensur', 1),\n",
       " (u'artifici', 1),\n",
       " (u'merg', 1),\n",
       " (u'institut', 1),\n",
       " (u'area', 3),\n",
       " (u'start', 2),\n",
       " (u'includ', 4),\n",
       " (u'way', 1),\n",
       " (u'gil', 1),\n",
       " (u'was', 6),\n",
       " (u'analysi', 6),\n",
       " (u'an', 6),\n",
       " (u'form', 1),\n",
       " (u'offer', 2),\n",
       " (u'forc', 1),\n",
       " (u'forb', 1),\n",
       " (u'but', 1),\n",
       " (u'idea', 1),\n",
       " (u'translat', 1),\n",
       " (u'with', 5),\n",
       " (u'conclus', 1),\n",
       " (u'he', 4),\n",
       " (u'consist', 1),\n",
       " (u'up', 1),\n",
       " (u'similar', 1),\n",
       " (u'display', 1),\n",
       " (u'classif', 2),\n",
       " (u'carver', 1),\n",
       " (u'evalu', 1),\n",
       " (u'lectur', 5),\n",
       " (u'novemb', 1),\n",
       " (u'as', 10),\n",
       " (u'exist', 1),\n",
       " (u'at', 3),\n",
       " (u'encompass', 2),\n",
       " (u'probabl', 1),\n",
       " (u'assembl', 1),\n",
       " (u'no', 2),\n",
       " (u'titl', 1),\n",
       " (u'field', 7),\n",
       " (u'other', 1),\n",
       " (u'communic', 1),\n",
       " (u'branch', 1),\n",
       " (u'devot', 1),\n",
       " (u'william', 1),\n",
       " (u'consid', 1),\n",
       " (u'appoint', 1),\n",
       " (u'algorithm', 1),\n",
       " (u'descript', 1),\n",
       " (u'c.f', 1),\n",
       " (u'emerg', 1),\n",
       " (u'time', 2),\n",
       " (u'international', 1)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################\n",
    "##### EXERCISE ####\n",
    "###################\n",
    "\n",
    "# Put each word in clean_tokens in lower case\n",
    "# find the new word count of the lowered tokens\n",
    "# Then show the top 10 words used in this corpus\n",
    "for item in sorted(c.items())[:25]:\n",
    "    print item[0].lower()\n",
    "\n",
    "    new_c = Counter(item[0].lower())\n",
    "    new_c.most_common(25)\n",
    "c.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "What:  Reduce a word to its base/stem form\n",
    "\n",
    "Why:   Often makes sense to treat multiple word forms the same way\n",
    "\n",
    "Notes: Uses a \"simple\" and fast rule-based approach\n",
    "       Output can be undesirable for irregular words\n",
    "       Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "       Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'charg'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example stemming\n",
    "stemmer.stem('charge')\n",
    "stemmer.stem('charging')\n",
    "stemmer.stem('charged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'data',\n",
       " u'scienc',\n",
       " u'is',\n",
       " u'an',\n",
       " u'interdisciplinari',\n",
       " u'field',\n",
       " u'about',\n",
       " u'process',\n",
       " u'and',\n",
       " u'system',\n",
       " u'to',\n",
       " u'extract',\n",
       " u'knowledg',\n",
       " u'or',\n",
       " u'insight',\n",
       " u'from',\n",
       " u'data',\n",
       " u'in',\n",
       " u'various',\n",
       " u'form',\n",
       " u'either',\n",
       " u'structur',\n",
       " u'or',\n",
       " u'unstructur',\n",
       " u'which',\n",
       " u'is',\n",
       " u'a',\n",
       " u'continu',\n",
       " u'of',\n",
       " u'some',\n",
       " u'of',\n",
       " u'the',\n",
       " u'data',\n",
       " u'analysi',\n",
       " u'field',\n",
       " u'such',\n",
       " u'as',\n",
       " u'statist',\n",
       " u'machin',\n",
       " u'learn',\n",
       " u'data',\n",
       " u'mine',\n",
       " u'and',\n",
       " u'predict',\n",
       " u'analyt',\n",
       " u'similar',\n",
       " u'to',\n",
       " u'knowledg',\n",
       " u'discoveri',\n",
       " u'in',\n",
       " u'databas',\n",
       " u'kdd',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'employ',\n",
       " u'techniqu',\n",
       " u'and',\n",
       " u'theori',\n",
       " u'drawn',\n",
       " u'from',\n",
       " u'mani',\n",
       " u'field',\n",
       " u'within',\n",
       " u'the',\n",
       " u'broad',\n",
       " u'area',\n",
       " u'of',\n",
       " u'mathemat',\n",
       " u'statist',\n",
       " u'oper',\n",
       " u'research',\n",
       " u'inform',\n",
       " u'scienc',\n",
       " u'and',\n",
       " u'comput',\n",
       " u'scienc',\n",
       " u'includ',\n",
       " u'signal',\n",
       " u'process',\n",
       " u'probabl',\n",
       " u'model',\n",
       " u'machin',\n",
       " u'learn',\n",
       " u'statist',\n",
       " u'learn',\n",
       " u'data',\n",
       " u'mine',\n",
       " u'databas',\n",
       " u'data',\n",
       " u'engin',\n",
       " u'pattern',\n",
       " u'recognit',\n",
       " u'and',\n",
       " u'learn',\n",
       " u'visual',\n",
       " u'predict',\n",
       " u'analyt',\n",
       " u'uncertainti',\n",
       " u'model',\n",
       " u'data',\n",
       " u'wareh',\n",
       " u'data',\n",
       " u'compress',\n",
       " u'comput',\n",
       " u'program',\n",
       " u'artifici',\n",
       " u'intellig',\n",
       " u'and',\n",
       " u'high',\n",
       " u'perform',\n",
       " u'comput',\n",
       " u'method',\n",
       " u'that',\n",
       " u'scale',\n",
       " u'to',\n",
       " u'big',\n",
       " u'data',\n",
       " u'are',\n",
       " u'of',\n",
       " u'particular',\n",
       " u'interest',\n",
       " u'in',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'although',\n",
       " u'the',\n",
       " u'disciplin',\n",
       " u'is',\n",
       " u'not',\n",
       " u'general',\n",
       " u'consid',\n",
       " u'to',\n",
       " u'be',\n",
       " u'restrict',\n",
       " u'to',\n",
       " u'such',\n",
       " u'big',\n",
       " u'data',\n",
       " u'and',\n",
       " u'big',\n",
       " u'data',\n",
       " u'technolog',\n",
       " u'are',\n",
       " u'often',\n",
       " u'focus',\n",
       " u'on',\n",
       " u'organ',\n",
       " u'and',\n",
       " u'preprocess',\n",
       " u'the',\n",
       " u'data',\n",
       " u'instead',\n",
       " u'of',\n",
       " u'analysi',\n",
       " u'the',\n",
       " u'develop',\n",
       " u'of',\n",
       " u'machin',\n",
       " u'learn',\n",
       " u'has',\n",
       " u'enhanc',\n",
       " u'the',\n",
       " u'growth',\n",
       " u'and',\n",
       " u'import',\n",
       " u'of',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'affect',\n",
       " u'academ',\n",
       " u'and',\n",
       " u'appli',\n",
       " u'research',\n",
       " u'in',\n",
       " u'mani',\n",
       " u'domain',\n",
       " u'includ',\n",
       " u'machin',\n",
       " u'translat',\n",
       " u'speech',\n",
       " u'recognit',\n",
       " u'robot',\n",
       " u'search',\n",
       " u'engin',\n",
       " u'digit',\n",
       " u'economi',\n",
       " u'but',\n",
       " u'also',\n",
       " u'the',\n",
       " u'biolog',\n",
       " u'scienc',\n",
       " u'medic',\n",
       " u'informat',\n",
       " u'health',\n",
       " u'care',\n",
       " u'social',\n",
       " u'scienc',\n",
       " u'and',\n",
       " u'the',\n",
       " u'human',\n",
       " u'it',\n",
       " u'heavili',\n",
       " u'influenc',\n",
       " u'econom',\n",
       " u'busi',\n",
       " u'and',\n",
       " u'financ',\n",
       " u'from',\n",
       " u'the',\n",
       " u'busi',\n",
       " u'perspect',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'is',\n",
       " u'an',\n",
       " u'integr',\n",
       " u'part',\n",
       " u'of',\n",
       " u'competit',\n",
       " u'intellig',\n",
       " u'a',\n",
       " u'newli',\n",
       " u'emerg',\n",
       " u'field',\n",
       " u'that',\n",
       " u'encompass',\n",
       " u'a',\n",
       " u'number',\n",
       " u'of',\n",
       " u'activ',\n",
       " u'such',\n",
       " u'as',\n",
       " u'data',\n",
       " u'mine',\n",
       " u'and',\n",
       " u'data',\n",
       " u'analysi',\n",
       " u'data',\n",
       " u'scientist',\n",
       " u'use',\n",
       " u'their',\n",
       " u'data',\n",
       " u'and',\n",
       " u'analyt',\n",
       " u'abil',\n",
       " u'to',\n",
       " u'find',\n",
       " u'and',\n",
       " u'interpret',\n",
       " u'rich',\n",
       " u'data',\n",
       " u'sourc',\n",
       " u'manag',\n",
       " u'larg',\n",
       " u'amount',\n",
       " u'of',\n",
       " u'data',\n",
       " u'despit',\n",
       " u'hardwar',\n",
       " u'softwar',\n",
       " u'and',\n",
       " u'bandwidth',\n",
       " u'constraint',\n",
       " u'merg',\n",
       " u'data',\n",
       " u'sourc',\n",
       " u'ensur',\n",
       " u'consist',\n",
       " u'of',\n",
       " u'dataset',\n",
       " u'creat',\n",
       " u'visual',\n",
       " u'to',\n",
       " u'aid',\n",
       " u'in',\n",
       " u'understand',\n",
       " u'data',\n",
       " u'build',\n",
       " u'mathemat',\n",
       " u'model',\n",
       " u'use',\n",
       " u'the',\n",
       " u'data',\n",
       " u'and',\n",
       " u'present',\n",
       " u'and',\n",
       " u'communic',\n",
       " u'the',\n",
       " u'data',\n",
       " u'insights/find',\n",
       " u'they',\n",
       " u'are',\n",
       " u'often',\n",
       " u'expect',\n",
       " u'to',\n",
       " u'produc',\n",
       " u'answer',\n",
       " u'in',\n",
       " u'day',\n",
       " u'rather',\n",
       " u'than',\n",
       " u'month',\n",
       " u'work',\n",
       " u'by',\n",
       " u'exploratori',\n",
       " u'analysi',\n",
       " u'and',\n",
       " u'rapid',\n",
       " u'iter',\n",
       " u'and',\n",
       " u'to',\n",
       " u'produc',\n",
       " u'and',\n",
       " u'present',\n",
       " u'result',\n",
       " u'with',\n",
       " u'dashboard',\n",
       " u'display',\n",
       " u'of',\n",
       " u'current',\n",
       " u'valu',\n",
       " u'rather',\n",
       " u'than',\n",
       " u'papers/report',\n",
       " u'as',\n",
       " u'statistician',\n",
       " u'normal',\n",
       " u'do',\n",
       " u'data',\n",
       " u'scientist',\n",
       " u'has',\n",
       " u'becom',\n",
       " u'a',\n",
       " u'popular',\n",
       " u'occup',\n",
       " u'with',\n",
       " u'harvard',\n",
       " u'busi',\n",
       " u'review',\n",
       " u'dub',\n",
       " u'it',\n",
       " u'the',\n",
       " u'sexiest',\n",
       " u'job',\n",
       " u'of',\n",
       " u'the',\n",
       " u'centuri',\n",
       " u'and',\n",
       " u'mckinsey',\n",
       " u'compani',\n",
       " u'project',\n",
       " u'a',\n",
       " u'global',\n",
       " u'excess',\n",
       " u'demand',\n",
       " u'of',\n",
       " u'million',\n",
       " u'new',\n",
       " u'data',\n",
       " u'scientist',\n",
       " u'univers',\n",
       " u'are',\n",
       " u'offer',\n",
       " u'master',\n",
       " u'cours',\n",
       " u'in',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'shorter',\n",
       " u'privat',\n",
       " u'bootcamp',\n",
       " u'are',\n",
       " u'also',\n",
       " u'offer',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'certif',\n",
       " u'includ',\n",
       " u'student-paid',\n",
       " u'program',\n",
       " u'like',\n",
       " u'general',\n",
       " u'assembl',\n",
       " u'to',\n",
       " u'employer-paid',\n",
       " u'program',\n",
       " u'like',\n",
       " u'the',\n",
       " u'data',\n",
       " u'incub',\n",
       " u'the',\n",
       " u'term',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'origin',\n",
       " u'use',\n",
       " u'interchang',\n",
       " u'with',\n",
       " u'datalog',\n",
       " u'has',\n",
       " u'exist',\n",
       " u'for',\n",
       " u'over',\n",
       " u'thirti',\n",
       " u'year',\n",
       " u'and',\n",
       " u'was',\n",
       " u'use',\n",
       " u'initi',\n",
       " u'as',\n",
       " u'a',\n",
       " u'substitut',\n",
       " u'for',\n",
       " u'comput',\n",
       " u'scienc',\n",
       " u'by',\n",
       " u'peter',\n",
       " u'naur',\n",
       " u'in',\n",
       " u'in',\n",
       " u'naur',\n",
       " u'publish',\n",
       " u'concis',\n",
       " u'survey',\n",
       " u'of',\n",
       " u'comput',\n",
       " u'method',\n",
       " u'which',\n",
       " u'freeli',\n",
       " u'use',\n",
       " u'the',\n",
       " u'term',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'in',\n",
       " u'it',\n",
       " u'survey',\n",
       " u'of',\n",
       " u'the',\n",
       " u'contemporari',\n",
       " u'data',\n",
       " u'process',\n",
       " u'method',\n",
       " u'that',\n",
       " u'are',\n",
       " u'use',\n",
       " u'in',\n",
       " u'a',\n",
       " u'wide',\n",
       " u'rang',\n",
       " u'of',\n",
       " u'applic',\n",
       " u'in',\n",
       " u'member',\n",
       " u'of',\n",
       " u'the',\n",
       " u'intern',\n",
       " u'feder',\n",
       " u'of',\n",
       " u'classif',\n",
       " u'societi',\n",
       " u'ifc',\n",
       " u'met',\n",
       " u'in',\n",
       " u'kobe',\n",
       " u'for',\n",
       " u'their',\n",
       " u'biennial',\n",
       " u'confer',\n",
       " u'here',\n",
       " u'for',\n",
       " u'the',\n",
       " u'first',\n",
       " u'time',\n",
       " u'the',\n",
       " u'term',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'is',\n",
       " u'includ',\n",
       " u'in',\n",
       " u'the',\n",
       " u'titl',\n",
       " u'of',\n",
       " u'the',\n",
       " u'confer',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'classif',\n",
       " u'and',\n",
       " u'relat',\n",
       " u'method',\n",
       " u'in',\n",
       " u'novemb',\n",
       " u'c.f',\n",
       " u'jeff',\n",
       " u'wu',\n",
       " u'gave',\n",
       " u'the',\n",
       " u'inaugur',\n",
       " u'lectur',\n",
       " u'entitl',\n",
       " u'statist',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'for',\n",
       " u'his',\n",
       " u'appoint',\n",
       " u'to',\n",
       " u'the',\n",
       " u'h.',\n",
       " u'c.',\n",
       " u'carver',\n",
       " u'professorship',\n",
       " u'at',\n",
       " u'the',\n",
       " u'univers',\n",
       " u'of',\n",
       " u'michigan',\n",
       " u'in',\n",
       " u'this',\n",
       " u'lectur',\n",
       " u'he',\n",
       " u'character',\n",
       " u'statist',\n",
       " u'work',\n",
       " u'as',\n",
       " u'a',\n",
       " u'trilog',\n",
       " u'of',\n",
       " u'data',\n",
       " u'collect',\n",
       " u'data',\n",
       " u'model',\n",
       " u'and',\n",
       " u'analysi',\n",
       " u'and',\n",
       " u'decis',\n",
       " u'make',\n",
       " u'in',\n",
       " u'his',\n",
       " u'conclus',\n",
       " u'he',\n",
       " u'initi',\n",
       " u'the',\n",
       " u'modern',\n",
       " u'non-comput',\n",
       " u'scienc',\n",
       " u'usag',\n",
       " u'of',\n",
       " u'the',\n",
       " u'term',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'and',\n",
       " u'advoc',\n",
       " u'that',\n",
       " u'statist',\n",
       " u'be',\n",
       " u'renam',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'and',\n",
       " u'statistician',\n",
       " u'data',\n",
       " u'scientist',\n",
       " u'later',\n",
       " u'he',\n",
       " u'present',\n",
       " u'his',\n",
       " u'lectur',\n",
       " u'entitl',\n",
       " u'statist',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'as',\n",
       " u'the',\n",
       " u'first',\n",
       " u'of',\n",
       " u'his',\n",
       " u'p.c',\n",
       " u'mahalanobi',\n",
       " u'memori',\n",
       " u'lectur',\n",
       " u'these',\n",
       " u'lectur',\n",
       " u'honor',\n",
       " u'prasanta',\n",
       " u'chandra',\n",
       " u'mahalanobi',\n",
       " u'an',\n",
       " u'indian',\n",
       " u'scientist',\n",
       " u'and',\n",
       " u'statistician',\n",
       " u'and',\n",
       " u'founder',\n",
       " u'of',\n",
       " u'the',\n",
       " u'indian',\n",
       " u'statist',\n",
       " u'institut',\n",
       " u'in',\n",
       " u'william',\n",
       " u's.',\n",
       " u'cleveland',\n",
       " u'introduc',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'as',\n",
       " u'an',\n",
       " u'independ',\n",
       " u'disciplin',\n",
       " u'extend',\n",
       " u'the',\n",
       " u'field',\n",
       " u'of',\n",
       " u'statist',\n",
       " u'to',\n",
       " u'incorpor',\n",
       " u'advanc',\n",
       " u'in',\n",
       " u'comput',\n",
       " u'with',\n",
       " u'data',\n",
       " u'in',\n",
       " u'his',\n",
       " u'articl',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'an',\n",
       " u'action',\n",
       " u'plan',\n",
       " u'for',\n",
       " u'expand',\n",
       " u'the',\n",
       " u'technic',\n",
       " u'area',\n",
       " u'of',\n",
       " u'the',\n",
       " u'field',\n",
       " u'of',\n",
       " u'statist',\n",
       " u'which',\n",
       " u'was',\n",
       " u'publish',\n",
       " u'in',\n",
       " u'volum',\n",
       " u'no',\n",
       " u'of',\n",
       " u'the',\n",
       " u'april',\n",
       " u'edit',\n",
       " u'of',\n",
       " u'the',\n",
       " u'intern',\n",
       " u'statist',\n",
       " u'review',\n",
       " u'revu',\n",
       " u'international',\n",
       " u'de',\n",
       " u'statistiqu',\n",
       " u'in',\n",
       " u'his',\n",
       " u'report',\n",
       " u'cleveland',\n",
       " u'establish',\n",
       " u'six',\n",
       " u'technic',\n",
       " u'area',\n",
       " u'which',\n",
       " u'he',\n",
       " u'believ',\n",
       " u'to',\n",
       " u'encompass',\n",
       " u'the',\n",
       " u'field',\n",
       " u'of',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'multidisciplinari',\n",
       " u'investig',\n",
       " u'model',\n",
       " u'and',\n",
       " u'method',\n",
       " u'for',\n",
       " u'data',\n",
       " u'comput',\n",
       " u'with',\n",
       " u'data',\n",
       " u'pedagogi',\n",
       " u'tool',\n",
       " u'evalu',\n",
       " u'and',\n",
       " u'theori',\n",
       " u'in',\n",
       " u'april',\n",
       " u'the',\n",
       " u'intern',\n",
       " u'council',\n",
       " u'for',\n",
       " u'scienc',\n",
       " u'committe',\n",
       " u'on',\n",
       " u'data',\n",
       " u'for',\n",
       " u'scienc',\n",
       " u'and',\n",
       " u'technolog',\n",
       " u'codata',\n",
       " u'start',\n",
       " u'the',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'journal',\n",
       " u'a',\n",
       " u'public',\n",
       " u'focus',\n",
       " u'on',\n",
       " u'issu',\n",
       " u'such',\n",
       " u'as',\n",
       " u'the',\n",
       " u'descript',\n",
       " u'of',\n",
       " u'data',\n",
       " u'system',\n",
       " u'their',\n",
       " u'public',\n",
       " u'on',\n",
       " u'the',\n",
       " u'internet',\n",
       " u'applic',\n",
       " u'and',\n",
       " u'legal',\n",
       " u'issu',\n",
       " u'short',\n",
       " u'thereaft',\n",
       " u'in',\n",
       " u'januari',\n",
       " u'columbia',\n",
       " u'univers',\n",
       " u'began',\n",
       " u'publish',\n",
       " u'the',\n",
       " u'journal',\n",
       " u'of',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'which',\n",
       " u'provid',\n",
       " u'a',\n",
       " u'platform',\n",
       " u'for',\n",
       " u'all',\n",
       " u'data',\n",
       " u'worker',\n",
       " u'to',\n",
       " u'present',\n",
       " u'their',\n",
       " u'view',\n",
       " u'and',\n",
       " u'exchang',\n",
       " u'idea',\n",
       " u'the',\n",
       " u'journal',\n",
       " u'was',\n",
       " u'larg',\n",
       " u'devot',\n",
       " u'to',\n",
       " u'the',\n",
       " u'applic',\n",
       " u'of',\n",
       " u'statist',\n",
       " u'method',\n",
       " u'and',\n",
       " u'quantit',\n",
       " u'research',\n",
       " u'in',\n",
       " u'the',\n",
       " u'nation',\n",
       " u'scienc',\n",
       " u'board',\n",
       " u'publish',\n",
       " u'long-liv',\n",
       " u'digit',\n",
       " u'data',\n",
       " u'collect',\n",
       " u'enabl',\n",
       " u'research',\n",
       " u'and',\n",
       " u'educ',\n",
       " u'in',\n",
       " u'the',\n",
       " u'centuri',\n",
       " u'defin',\n",
       " u'data',\n",
       " u'scientist',\n",
       " u'as',\n",
       " u'the',\n",
       " u'inform',\n",
       " u'and',\n",
       " u'comput',\n",
       " u'scientist',\n",
       " u'databas',\n",
       " u'and',\n",
       " u'softwar',\n",
       " u'and',\n",
       " u'programm',\n",
       " u'disciplinari',\n",
       " u'expert',\n",
       " u'curat',\n",
       " u'and',\n",
       " u'expert',\n",
       " u'annot',\n",
       " u'librarian',\n",
       " u'archivist',\n",
       " u'and',\n",
       " u'other',\n",
       " u'who',\n",
       " u'are',\n",
       " u'crucial',\n",
       " u'to',\n",
       " u'the',\n",
       " u'success',\n",
       " u'manag',\n",
       " u'of',\n",
       " u'a',\n",
       " u'digit',\n",
       " u'data',\n",
       " u'collect',\n",
       " u'whose',\n",
       " u'primari',\n",
       " u'activ',\n",
       " u'is',\n",
       " u'to',\n",
       " u'conduct',\n",
       " u'creativ',\n",
       " u'inquiri',\n",
       " u'and',\n",
       " u'analysi',\n",
       " u'in',\n",
       " u'the',\n",
       " u'ieee',\n",
       " u'task',\n",
       " u'forc',\n",
       " u'on',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'and',\n",
       " u'advanc',\n",
       " u'analyt',\n",
       " u'was',\n",
       " u'launch',\n",
       " u'and',\n",
       " u'the',\n",
       " u'first',\n",
       " u'intern',\n",
       " u'confer',\n",
       " u'ieee',\n",
       " u'intern',\n",
       " u'confer',\n",
       " u'on',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'and',\n",
       " u'advanc',\n",
       " u'analyt',\n",
       " u'was',\n",
       " u'launch',\n",
       " u'in',\n",
       " u'in',\n",
       " u'the',\n",
       " u'intern',\n",
       " u'journal',\n",
       " u'on',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'and',\n",
       " u'analyt',\n",
       " u'was',\n",
       " u'launch',\n",
       " u'by',\n",
       " u'springer',\n",
       " u'to',\n",
       " u'publish',\n",
       " u'origin',\n",
       " u'work',\n",
       " u'on',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'and',\n",
       " u'big',\n",
       " u'data',\n",
       " u'analyt',\n",
       " u'in',\n",
       " u'citat',\n",
       " u'need',\n",
       " u'dj',\n",
       " u'patil',\n",
       " u'and',\n",
       " u'jeff',\n",
       " u'hammerbach',\n",
       " u'use',\n",
       " u'the',\n",
       " u'term',\n",
       " u'data',\n",
       " u'scientist',\n",
       " u'to',\n",
       " u'defin',\n",
       " u'their',\n",
       " u'job',\n",
       " u'at',\n",
       " u'linkedin',\n",
       " u'and',\n",
       " u'facebook',\n",
       " u'respect',\n",
       " u'although',\n",
       " u'use',\n",
       " u'of',\n",
       " u'the',\n",
       " u'term',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'has',\n",
       " u'explod',\n",
       " u'in',\n",
       " u'busi',\n",
       " u'environ',\n",
       " u'mani',\n",
       " u'academ',\n",
       " u'and',\n",
       " u'journalist',\n",
       " u'see',\n",
       " u'no',\n",
       " u'distinct',\n",
       " u'between',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'and',\n",
       " u'statist',\n",
       " u'write',\n",
       " u'in',\n",
       " u'forb',\n",
       " u'gil',\n",
       " u'press',\n",
       " u'argu',\n",
       " u'that',\n",
       " u'data',\n",
       " u'scienc',\n",
       " u'is',\n",
       " u'a',\n",
       " u'buzzword',\n",
       " u'without',\n",
       " u'a',\n",
       " u'clear',\n",
       " u'definit',\n",
       " u'and',\n",
       " u'has',\n",
       " u'simpli',\n",
       " u'replac',\n",
       " u'analytics\\u201d',\n",
       " u'in',\n",
       " u'context',\n",
       " u'such',\n",
       " u'as',\n",
       " u'graduat',\n",
       " u'degre',\n",
       " u'program',\n",
       " u'in',\n",
       " u'the',\n",
       " u'question-and-answ',\n",
       " u'section',\n",
       " u'of',\n",
       " u'his',\n",
       " u'keynot',\n",
       " u'address',\n",
       " u'at',\n",
       " u'the',\n",
       " u'joint',\n",
       " u'statist',\n",
       " u'meet',\n",
       " u'of',\n",
       " u'american',\n",
       " u'statist',\n",
       " u'associ',\n",
       " u'note',\n",
       " u'appli',\n",
       " u'statistician',\n",
       " u'nate',\n",
       " u'silver',\n",
       " u'said',\n",
       " u'think',\n",
       " u'data-scientist',\n",
       " u'is',\n",
       " u'a',\n",
       " u'sex',\n",
       " u'up',\n",
       " ...]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stem the tokens\n",
    "stemmed_tokens = [stemmer.stem(t) for t in clean_tokens]\n",
    "stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'a', 16),\n",
       " (u'abil', 1),\n",
       " (u'about', 1),\n",
       " (u'academ', 2),\n",
       " (u'action', 1),\n",
       " (u'activ', 2),\n",
       " (u'address', 1),\n",
       " (u'advanc', 3),\n",
       " (u'advoc', 1),\n",
       " (u'affect', 1),\n",
       " (u'aid', 1),\n",
       " (u'algorithm', 1),\n",
       " (u'all', 1),\n",
       " (u'allow', 1),\n",
       " (u'also', 2),\n",
       " (u'although', 2),\n",
       " (u'american', 1),\n",
       " (u'amount', 1),\n",
       " (u'an', 6),\n",
       " (u'analysi', 6),\n",
       " (u'analyt', 7),\n",
       " (u'analytics\\u201d', 1),\n",
       " (u'and', 56),\n",
       " (u'annot', 1),\n",
       " (u'answer', 1)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the stemmed tokens\n",
    "c = Counter(stemmed_tokens)\n",
    "c.most_common(25)       # all lowercase\n",
    "sorted(c.items())[:25]  # some are strange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "What:  Derive the canonical form ('lemma') of a word\n",
    "    \n",
    "Why:   Can be better than stemming, reduces words to a 'normal' form.\n",
    "    \n",
    "Notes: Uses a dictionary-based approach (slower than stemming)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'dog'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "# compare stemmer to lemmatizer\n",
    "stemmer.stem('dogs')\n",
    "lemmatizer.lemmatize('dogs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'wolf'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('wolves') # Beter for information retrieval and search\n",
    "lemmatizer.lemmatize('wolves') # Better for text analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'be'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('is')\n",
    "lemmatizer.lemmatize('is')\n",
    "lemmatizer.lemmatize('is',pos='v')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging\n",
    "\n",
    "What:  Determine the part of speech of a word\n",
    "    \n",
    "Why:   This can inform other methods and models such as Named Entity Recognition\n",
    "    \n",
    "Notes: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('revision', 'NN'),\n",
       " ('class', 'NN'),\n",
       " ('this', 'DT'),\n",
       " ('Saturday', 'NNP')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_sent = 'We have a revision class this Saturday'\n",
    "# pos_tag takes a tokenize sentence\n",
    "nltk.pos_tag(nltk.word_tokenize(temp_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "\n",
    "What:  Remove common words that will likely appear in any text\n",
    "    \n",
    "Why:   They don't tell you much about your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'data', 69),\n",
       " (u'the', 58),\n",
       " (u'and', 56),\n",
       " (u'of', 41),\n",
       " (u'scienc', 39),\n",
       " (u'in', 36),\n",
       " (u'to', 19),\n",
       " (u'a', 16),\n",
       " (u'statist', 15),\n",
       " (u'for', 11),\n",
       " (u'is', 10),\n",
       " (u'as', 10),\n",
       " (u'use', 9),\n",
       " (u'scientist', 9),\n",
       " (u'term', 8),\n",
       " (u'comput', 8),\n",
       " (u'on', 8),\n",
       " (u'softwar', 7),\n",
       " (u'analyt', 7),\n",
       " (u'his', 7),\n",
       " (u'are', 7),\n",
       " (u'field', 7),\n",
       " (u'method', 6),\n",
       " (u'intern', 6),\n",
       " (u'was', 6)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most of top 25 stemmed tokens are \"worthless\"\n",
    "c.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'a',\n",
       " u'about',\n",
       " u'above',\n",
       " u'after',\n",
       " u'again',\n",
       " u'against',\n",
       " u'ain',\n",
       " u'all',\n",
       " u'am',\n",
       " u'an',\n",
       " u'and',\n",
       " u'any',\n",
       " u'are',\n",
       " u'aren',\n",
       " u'as',\n",
       " u'at',\n",
       " u'be',\n",
       " u'because',\n",
       " u'been',\n",
       " u'before',\n",
       " u'being',\n",
       " u'below',\n",
       " u'between',\n",
       " u'both',\n",
       " u'but',\n",
       " u'by',\n",
       " u'can',\n",
       " u'couldn',\n",
       " u'd',\n",
       " u'did',\n",
       " u'didn',\n",
       " u'do',\n",
       " u'does',\n",
       " u'doesn',\n",
       " u'doing',\n",
       " u'don',\n",
       " u'down',\n",
       " u'during',\n",
       " u'each',\n",
       " u'few',\n",
       " u'for',\n",
       " u'from',\n",
       " u'further',\n",
       " u'had',\n",
       " u'hadn',\n",
       " u'has',\n",
       " u'hasn',\n",
       " u'have',\n",
       " u'haven',\n",
       " u'having',\n",
       " u'he',\n",
       " u'her',\n",
       " u'here',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'him',\n",
       " u'himself',\n",
       " u'his',\n",
       " u'how',\n",
       " u'i',\n",
       " u'if',\n",
       " u'in',\n",
       " u'into',\n",
       " u'is',\n",
       " u'isn',\n",
       " u'it',\n",
       " u'its',\n",
       " u'itself',\n",
       " u'just',\n",
       " u'll',\n",
       " u'm',\n",
       " u'ma',\n",
       " u'me',\n",
       " u'mightn',\n",
       " u'more',\n",
       " u'most',\n",
       " u'mustn',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'needn',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'now',\n",
       " u'o',\n",
       " u'of',\n",
       " u'off',\n",
       " u'on',\n",
       " u'once',\n",
       " u'only',\n",
       " u'or',\n",
       " u'other',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'out',\n",
       " u'over',\n",
       " u'own',\n",
       " u're',\n",
       " u's',\n",
       " u'same',\n",
       " u'shan',\n",
       " u'she',\n",
       " u'should',\n",
       " u'shouldn',\n",
       " u'so',\n",
       " u'some',\n",
       " u'such',\n",
       " u't',\n",
       " u'than',\n",
       " u'that',\n",
       " u'the',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'them',\n",
       " u'themselves',\n",
       " u'then',\n",
       " u'there',\n",
       " u'these',\n",
       " u'they',\n",
       " u'this',\n",
       " u'those',\n",
       " u'through',\n",
       " u'to',\n",
       " u'too',\n",
       " u'under',\n",
       " u'until',\n",
       " u'up',\n",
       " u've',\n",
       " u'very',\n",
       " u'was',\n",
       " u'wasn',\n",
       " u'we',\n",
       " u'were',\n",
       " u'weren',\n",
       " u'what',\n",
       " u'when',\n",
       " u'where',\n",
       " u'which',\n",
       " u'while',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'why',\n",
       " u'will',\n",
       " u'with',\n",
       " u'won',\n",
       " u'wouldn',\n",
       " u'y',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the list of stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "sorted(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "### Exercise  ####\n",
    "##################\n",
    "\n",
    "\n",
    "# Create a variable called stemmed_stops which is the \n",
    "# stemmed version of each stopword in stopwords\n",
    "# Use the stemmer we used up above!\n",
    "\n",
    "# Then create a list called stemmed_tokens_no_stop that \n",
    "# contains only the tokens in stemmed_tokens that aren't in \n",
    "# stemmed_stops\n",
    "\n",
    "# Show the 25 most common stemmed non stop word tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "What:  Automatically extract the names of people, places, organizations, etc.\n",
    "\n",
    "Why:   Can help you to identify \"important\" words\n",
    "\n",
    "Notes: Training NER classifier requires a lot of annotated training data\n",
    "       Should be trained on data relevant to your task\n",
    "       Stanford NER classifier is the \"gold standard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alasdair', 'is', 'an', 'instructor', 'for', 'General', 'Assembly']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Alasdair is an instructor for General Assembly'\n",
    "\n",
    "tokenized = nltk.word_tokenize(sentence)\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alasdair', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('instructor', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('General', 'NNP'),\n",
       " ('Assembly', 'NNP')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "tagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('GPE', [('Alasdair', 'NNP')]), ('is', 'VBZ'), ('an', 'DT'), ('instructor', 'NN'), ('for', 'IN'), Tree('ORGANIZATION', [('General', 'NNP'), ('Assembly', 'NNP')])])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = nltk.ne_chunk(tagged)\n",
    "\n",
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPE] Alasdair\n",
      "[ORGANIZATION] General Assembly\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(text):\n",
    "    entities = []\n",
    "    # tokenize into sentences\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        # tokenize sentences into words\n",
    "        # add part-of-speech tags\n",
    "        # use NLTK's NER classifier\n",
    "        chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "        # parse the results\n",
    "        entities.extend([chunk for chunk in chunks if hasattr(chunk, 'label')])\n",
    "    return entities\n",
    "\n",
    "for entity in extract_entities('Alasdair is an instructor for General Assembly'):\n",
    "    print '[' + entity.label() + '] ' + ' '.join(c[0] for c in entity.leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "What:  Computes \"relative frequency\" that a word appears in a document\n",
    "           compared to its frequency across all documents\n",
    "\n",
    "Why:   More useful than \"term frequency\" for identifying \"important\" words in\n",
    "           each document (high frequency in that document, low frequency in\n",
    "           other documents)\n",
    "\n",
    "Notes: Used for search engine scoring, text summarization, document clustering\n",
    "\n",
    "How: \n",
    "    TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "    IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = ['Bob likes sports', 'Bob hates sports', 'Bob likes likes trees']\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'bob', u'hates', u'likes', u'sports', u'trees']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each row represents a sentence\n",
    "# Each column represents a word\n",
    "vect.fit_transform(sample).toarray()\n",
    "vect.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'bob', u'hates', u'likes', u'sports', u'trees']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(sample).toarray()\n",
    "tfidf.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'bob': 1.0, u'trees': 1.6931471805599454, u'likes': 1.2876820724517808, u'hates': 1.6931471805599454, u'sports': 1.2876820724517808}\n"
     ]
    }
   ],
   "source": [
    "# the IDF of each word\n",
    "idf = tfidf.idf_\n",
    "print dict(zip(tfidf.get_feature_names(), idf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "## Exercise ###\n",
    "###############\n",
    "\n",
    "\n",
    "# for each sentence in sample, find the most \"interesting \n",
    "#words\" by ordering their tfidf in ascending order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA - Latent Dirichlet Allocation\n",
    "\n",
    "What:  Way of automatically discovering topics from sentences\n",
    "\n",
    "Why:   Much quicker than manually creating and identifying topic clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lda\n",
      "  Downloading lda-1.0.4-cp27-cp27m-win_amd64.whl (313kB)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy<2.0,>=1.6.1 in c:\\users\\wosim\\appdata\\local\\continuum\\anaconda2\\lib\\site-packages (from lda)\n",
      "Collecting pbr>=0.6 (from lda)\n",
      "  Downloading pbr-1.10.0-py2.py3-none-any.whl (96kB)\n",
      "Installing collected packages: pbr, lda\n",
      "Successfully installed lda-1.0.4 pbr-1.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lda\n",
    "\n",
    "# Instantiate a count vectorizer with two additional parameters\n",
    "vect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \n",
    "sentences_train = vect.fit_transform(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 41\n",
      "INFO:lda:vocab_size: 1675\n",
      "INFO:lda:n_words: 2038\n",
      "INFO:lda:n_topics: 10\n",
      "INFO:lda:n_iter: 500\n",
      "INFO:lda:<0> log likelihood: -23525\n",
      "INFO:lda:<10> log likelihood: -20343\n",
      "INFO:lda:<20> log likelihood: -19911\n",
      "INFO:lda:<30> log likelihood: -19588\n",
      "INFO:lda:<40> log likelihood: -19376\n",
      "INFO:lda:<50> log likelihood: -19515\n",
      "INFO:lda:<60> log likelihood: -19430\n",
      "INFO:lda:<70> log likelihood: -19526\n",
      "INFO:lda:<80> log likelihood: -19591\n",
      "INFO:lda:<90> log likelihood: -19412\n",
      "INFO:lda:<100> log likelihood: -19224\n",
      "INFO:lda:<110> log likelihood: -19293\n",
      "INFO:lda:<120> log likelihood: -19360\n",
      "INFO:lda:<130> log likelihood: -19151\n",
      "INFO:lda:<140> log likelihood: -19303\n",
      "INFO:lda:<150> log likelihood: -19233\n",
      "INFO:lda:<160> log likelihood: -19133\n",
      "INFO:lda:<170> log likelihood: -19312\n",
      "INFO:lda:<180> log likelihood: -19283\n",
      "INFO:lda:<190> log likelihood: -19307\n",
      "INFO:lda:<200> log likelihood: -19172\n",
      "INFO:lda:<210> log likelihood: -19305\n",
      "INFO:lda:<220> log likelihood: -19202\n",
      "INFO:lda:<230> log likelihood: -19135\n",
      "INFO:lda:<240> log likelihood: -19231\n",
      "INFO:lda:<250> log likelihood: -19246\n",
      "INFO:lda:<260> log likelihood: -19158\n",
      "INFO:lda:<270> log likelihood: -19129\n",
      "INFO:lda:<280> log likelihood: -19186\n",
      "INFO:lda:<290> log likelihood: -19348\n",
      "INFO:lda:<300> log likelihood: -19322\n",
      "INFO:lda:<310> log likelihood: -19338\n",
      "INFO:lda:<320> log likelihood: -19151\n",
      "INFO:lda:<330> log likelihood: -19337\n",
      "INFO:lda:<340> log likelihood: -19147\n",
      "INFO:lda:<350> log likelihood: -19250\n",
      "INFO:lda:<360> log likelihood: -19306\n",
      "INFO:lda:<370> log likelihood: -19156\n",
      "INFO:lda:<380> log likelihood: -19260\n",
      "INFO:lda:<390> log likelihood: -19217\n",
      "INFO:lda:<400> log likelihood: -19273\n",
      "INFO:lda:<410> log likelihood: -19329\n",
      "INFO:lda:<420> log likelihood: -19291\n",
      "INFO:lda:<430> log likelihood: -19170\n",
      "INFO:lda:<440> log likelihood: -19260\n",
      "INFO:lda:<450> log likelihood: -19150\n",
      "INFO:lda:<460> log likelihood: -19267\n",
      "INFO:lda:<470> log likelihood: -19216\n",
      "INFO:lda:<480> log likelihood: -19298\n",
      "INFO:lda:<490> log likelihood: -19159\n",
      "INFO:lda:<499> log likelihood: -19318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: research, statistical, digital, information, collection, digital data, data collection, sciences, recognition\n",
      "Topic 1: data, data science, science, international, conference, analytics, launched, issues, systems\n",
      "Topic 2: computer, term, term data, science, term data science, used, methods, computer science, university\n",
      "Topic 3: programs, analytics, april, paid, programs like, statisticians, paid programs, paid programs like, like\n",
      "Topic 4: software, use, open source software, sources, source software, source, insights, present, review\n",
      "Topic 5: learning, field, mining, computing, data mining, areas, intelligence, technical areas, fields\n",
      "Topic 6: analysis, machine, big, journal, work, big data, learning data mining, journal data, machine learning\n",
      "Topic 7: data, scientists, data scientists, published, processing, data sources, 21st, century, 21st century\n",
      "Topic 8: scientist, data scientist, statistician, statistical, indian, term, lectures, mahalanobis, term statistician\n",
      "Topic 9: data, science, data science, statistics, business, lecture, models, including, applied\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  5.35475234e-05,   5.35475234e-05,   5.35475234e-05, ...,\n",
       "          5.35475234e-05,   5.35475234e-05,   5.35475234e-05],\n",
       "       [  3.77714825e-05,   3.77714825e-05,   3.77714825e-05, ...,\n",
       "          3.77714825e-05,   3.77714825e-05,   3.77714825e-05],\n",
       "       [  4.95705521e-03,   4.95705521e-03,   4.95705521e-03, ...,\n",
       "          4.95705521e-03,   4.95705521e-03,   4.95705521e-03],\n",
       "       ..., \n",
       "       [  4.55062571e-05,   4.55062571e-05,   4.55062571e-05, ...,\n",
       "          4.55062571e-05,   4.55062571e-05,   4.55062571e-05],\n",
       "       [  4.72255018e-05,   4.72255018e-05,   4.72255018e-05, ...,\n",
       "          4.72255018e-05,   4.72255018e-05,   4.72255018e-05],\n",
       "       [  3.22841001e-05,   3.22841001e-05,   3.22841001e-05, ...,\n",
       "          3.22841001e-05,   3.22841001e-05,   3.22841001e-05]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate an LDA model\n",
    "model = lda.LDA(n_topics=10, n_iter=500)\n",
    "model.fit(sentences_train) # Fit the model \n",
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vect.get_feature_names())[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ', '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXAMPLE: Automatically summarize a document\n",
    "\n",
    "\n",
    "# corpus of 2000 movie reviews\n",
    "from nltk.corpus import movie_reviews\n",
    "reviews = [movie_reviews.raw(filename) for filename in movie_reviews.fileids()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create document-term matrix\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "dtm = tfidf.fit_transform(reviews)\n",
    "features = tfidf.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOWEST:\n",
      "\n",
      "it features an idiotic script , a hack director , and a one-dimensional star who is past his prime .\n",
      "resulting in his sometimes experiencing pain and sometimes nothing , as if the director forgot which way he wants to have it .\n",
      "it's just dreary and unwatchable .\n",
      "\n",
      "HIGHEST:\n",
      "\n",
      "it is now up to arnold to save the world and find the chosen woman .\n",
      "satan's numeral is actually \" 999 , '' which is upside down of \" 666 . ''\n",
      "he quickly finds christine and does everything to protect her from all the evil forces around her , as the film includes ludicrous helicopter rescues , people jumping out of windows , satan taking a piss and dropping a match to kill arnold's partner as the piss explodes like gasoline , satan being fired at .\n"
     ]
    }
   ],
   "source": [
    "# find the most and least \"interesting\" sentences in a randomly selected review\n",
    "def summarize():\n",
    "    \n",
    "    # choose a random movie review    \n",
    "    review_id = np.random.randint(0, len(reviews))\n",
    "    review_text = reviews[review_id]\n",
    "\n",
    "    # we are going to score each sentence in the review for \"interesting-ness\"\n",
    "    sent_scores = []\n",
    "    # tokenize document into sentences\n",
    "    for sentence in nltk.sent_tokenize(review_text):\n",
    "        # exclude short sentences\n",
    "        if len(sentence) > 6:\n",
    "            score = 0\n",
    "            token_count = 0\n",
    "            # tokenize sentence into words\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            # compute sentence \"score\" by summing TFIDF for each word\n",
    "            for token in tokens:\n",
    "                if token in features:\n",
    "                    score += dtm[review_id, features.index(token)]\n",
    "                    token_count += 1\n",
    "            # divide score by number of tokens\n",
    "            sent_scores.append((score / float(token_count + 1), sentence))\n",
    "\n",
    "    # lowest scoring sentences\n",
    "    print '\\nLOWEST:\\n'\n",
    "    for sent_score in sorted(sent_scores)[:3]:\n",
    "        print sent_score[1]\n",
    "\n",
    "    # highest scoring sentences\n",
    "    print '\\nHIGHEST:\\n'\n",
    "    for sent_score in sorted(sent_scores, reverse=True)[:3]:\n",
    "        print sent_score[1]\n",
    "\n",
    "# try it out!\n",
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): textblob in c:\\users\\wosim\\appdata\\local\\continuum\\anaconda2\\lib\\site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): nltk>=3.1 in c:\\users\\wosim\\appdata\\local\\continuum\\anaconda2\\lib\\site-packages (from textblob)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# TextBlob Demo: \"Simplified Text Processing\"\n",
    "# Installation: pip install textblob\n",
    "! pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['liam', 'sinan', u'general assembly'])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify words and noun phrases\n",
    "blob = TextBlob('Liam and Sinan are instructors for General Assembly')\n",
    "blob.words\n",
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.9, -0.26923076923076916]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment analysis\n",
    "blob = TextBlob('I hate this horrible movie. This movie is not very good.')\n",
    "blob.sentences\n",
    "blob.sentiment.polarity\n",
    "[sent.sentiment.polarity for sent in blob.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# sentiment subjectivity\n",
    "TextBlob(\"I am a cool person\").sentiment.subjectivity # Pretty subjective\n",
    "TextBlob(\"I am a person\").sentiment.subjectivity # Pretty objective\n",
    "# different scores for essentially the same sentence\n",
    "print TextBlob('Ian and Alasdair are instructors for General Assembly in Sydney').sentiment.subjectivity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Put', 'away', 'the', 'dish']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# singularize and pluralize\n",
    "blob = TextBlob('Put away the dishes.')\n",
    "[word.singularize() for word in blob.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Puts', 'aways', 'thes', 'dishess']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.pluralize() for word in blob.words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"15 minutes late\")"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spelling correction\n",
    "blob = TextBlob('15 minuets late')\n",
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('part', 0.9929478138222849), (u'parrot', 0.007052186177715092)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spellcheck\n",
    "Word('parot').spellcheck()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'tip laterally',\n",
       " u'enclose with a bank',\n",
       " u'do business with a bank or keep an account at a bank',\n",
       " u'act as the banker in a game or in gambling',\n",
       " u'be in the banking business',\n",
       " u'put into a bank account',\n",
       " u'cover with ashes so to control the rate of burning',\n",
       " u'have confidence or faith in']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definitions\n",
    "Word('bank').define()\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 503: Service Unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-e508e7be89b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# translation and language identification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Welcome to the classroom.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mblob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'es'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Hola amigos'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mblob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect_language\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\wosim\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\textblob\\blob.pyc\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, from_lang, to)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \"\"\"\n\u001b[1;32m    504\u001b[0m         return self.__class__(self.translator.translate(self.raw,\n\u001b[0;32m--> 505\u001b[0;31m                               from_lang=from_lang, to_lang=to))\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdetect_language\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\wosim\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\textblob\\translate.pyc\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, source, from_lang, to_lang, host, type_)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[1;34m\"dt\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"at\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tk\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_calculate_tk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \"sl\": from_lang, \"tl\": to_lang, \"text\": source}\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\wosim\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\textblob\\translate.pyc\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, url, host, type_, data)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhost\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtype_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_proxy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\wosim\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\wosim\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\wosim\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 548\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\wosim\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mhttp_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\wosim\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\wosim\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36mhttp_error_302\u001b[0;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0mhttp_error_301\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhttp_error_303\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhttp_error_307\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhttp_error_302\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\wosim\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\wosim\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 548\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\wosim\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\wosim\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\wosim\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_full_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 503: Service Unavailable"
     ]
    }
   ],
   "source": [
    "# translation and language identification\n",
    "blob = TextBlob('Welcome to the classroom.')\n",
    "blob.translate(to='es')\n",
    "blob = TextBlob('Hola amigos')\n",
    "blob.detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 7. Regular Expressions - Regex\n",
    "\n",
    "This is the python module for regular expressions: https://docs.python.org/2/library/re.html\n",
    "\n",
    "Here is a google page for explaining regular expression patterns: https://developers.google.com/edu/python/regular-expressions\n",
    "\n",
    "And here is a convenient tool for testing regular expressions: https://regex101.com/#python\n",
    "\n",
    "Have a read of these and play around with regular expressions below and in the regex101 tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
