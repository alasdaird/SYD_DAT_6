{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYD DAT 4 Lab 2 - Visualisation and Regression\n",
    "\n",
    "##Homework - Due 29th April 2016\n",
    "\n",
    "#### Setup\n",
    "* Signup for an AWS account\n",
    "\n",
    "#### Communication\n",
    "* Imagine you are trying to explain to someone what Linear Regression is - but they have no programming/maths experience? How would you explain the overall process, what a p-value means and what R-Squared means?\n",
    "* Read the paper [Useful things to know about machine learning]( https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf). \n",
    "    * What have we covered so far from this paper? \n",
    "    * Explain sections 6-13 in your own words\n",
    "\n",
    "#### Machine Learning\n",
    "* Read chapters 3 and 6 of Introduction to Statistical Learning\n",
    "* Describe 3 ways we can select what features to use in a model\n",
    "* Complete the first 3 exercises from Chapter 3 in Python\n",
    "\n",
    "#### Course Project\n",
    "* For the following setup a new github repository for your project and share it with Matt and Ian over Slack.\n",
    "* Load the data you have gathered for your project into Python and run some summary statistics over the data. Are there any interesting features of the data that jump out? (Include the code)\n",
    "* Draft/Sketch on paper (or wireframe) some data visualisations that would be useful for you to explore your data set\n",
    "* Are there any regresion or clustering techniques you could use in your project? Write them down (with the corresponding scikit learn function) and what you think you would get out of it.\n",
    "\n",
    "\n",
    "**Instructions: copy this file and append your name in the filename, e.g. Homework2_ian_hansel.ipynb.\n",
    "Then commit this in your local repository, push it to your github account and create a pull request so I can see your work. Remeber if you get stuck to look at the slides going over Fork, Clone, Commit, Push and Pull request.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Communication\n",
    "\n",
    "#Imagine you are trying to explain to someone what Linear Regression is - but they have no programming/maths experience? How would you explain the overall process, what a p-value means and what R-Squared means?\n",
    "\n",
    "    #Linear regression aims to mathematically identify and subsequently describe a relationship between two sets of data. Once the relationship has been identified and quantified, we can predict changes to one set of data based on movements in the other set of data.\n",
    "    \n",
    "    #The overall process involves\n",
    "        -obtaining the data sets of interest.\n",
    "        -identifying which data you would like to explain/predict (Call this 'A') and which data you would like to\n",
    "        -utlise for the explanation/prediciton (call this B, which can contain multiple elements)\n",
    "        -use mathematical techniques applied to the data set on hand to form a mathematical relationship between B and\n",
    "        -the element(s) in B\n",
    "        -use some simple statistical tests to determine the quality of your model, how strong the relationship between\n",
    "        -elements in B are with A.\n",
    "    #P-values and R^2 are two tests used to describe the strength of the relationship between B & A.\n",
    "        -P-Values use statistical tests to determine a value 'p' for the elements in B used to describe A. The value p\n",
    "         describes the likelihood of seeing a substantial association between the elements of A & B if we run the tests\n",
    "         numerous times. A large value for p indicates there is a large likelihood, a small value indicates the\n",
    "         relationship we deduced was due to chance.\n",
    "        -R^2 is a metric which describes the explanatory power of your model with elements of B to describe A. It\n",
    "        indicates the proportion of B which can be explained by A. A high value for R^s indicates a strong model.\n",
    "\n",
    "\n",
    "#What have we covered so far from this paper? \n",
    "#Explain sections 6-13 in your own words\n",
    "\n",
    "    #items covered mentioned in the paper are\n",
    "        -Cross validation\n",
    "        -Statistical significance testing\n",
    "        -bias/variance trade off\n",
    "        -overfitting\n",
    "        -regularisation\n",
    "        -fixed size representation learners (linear regression)\n",
    "\n",
    "#Explain sections 6-13 in your own words\n",
    "\n",
    "    #curse of dimensionality, in higher dimensions, beyond the 3rd, most data is is distributed in a shell around the\n",
    "     mean.\n",
    "    #in higher dimensions, many classifiers in a model will not add much significance to the model and cloud the\n",
    "     signals provided by  those significant classifiers.\n",
    "    #Adding many classifers to a model can work against the model as per the \"curse of dimentionality\".\n",
    "    #However this can be negated by the \"blessing of non-uniformity\" where there is clustering of data around lower\n",
    "    dimensions.\n",
    "    \n",
    "    #theoretical guarantees in machine learning should not be used for decision making, but as a source of\n",
    "    understanding and developing algorithmns.\n",
    "    \n",
    "    #raw data is not in a format that can be read directly. A process of obtaining, cleaning, pre processing\n",
    "    & identifying features is what feature engineering involves.\n",
    "    \n",
    "    # as a rule of thumb, in variable learners like decision trees, the more data you have, the better the model\n",
    "    will be. However a tradeoff between data & processing time may limit the benefits of gathering more data.\n",
    "    #Linear classifiers such as linear regression benefit less and less as more data is fed into the model.\n",
    "    \n",
    "    #Bagging is a technique which generates random sets of training data, obtains classifers, then combines the\n",
    "    results over multiple re-samples. Model ensembles have proven to be complex models which combine leaners from\n",
    "    many models to form one model. These have proven to be very successful as per the Netflix prize competition.\n",
    "    \n",
    "    #while you can extract causal information from learning algorithms, only limited information can be obtained.\n",
    "    learning models are used as a guide for action.\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Machine Learning\n",
    "\n",
    "#Describe 3 ways we can select what features to use in a model\n",
    "    \n",
    "#Subset Method: progressively add features to a model, and note the added explanatory power of the feature. \n",
    "#Once all features have been added, then only select those features which added significant explanatory power to the model\n",
    "\n",
    "#regularisation (Lasso & Ridge): These are similar to OLS, but integrate a penalty function which contrains the size of \n",
    "#the coefficients in the model. the higher the penalty paramater (lambda), the greater the shrinkage effect is on the model coefficients\n",
    "#this allows us to see which features have the most significance. In the case of Lasso, paramters can be shrunken to zero, while in\n",
    "#ridge regression the paramters are taken very close to zero, but don't become exactly zero. In  both methods, Theres a significant reduction\n",
    "#in the variance with only a slight increase in the bias.\n",
    "\n",
    "#Dimention Reduction (PCA): Involves, transforming the data into components which explain most of the variance of the data set\n",
    "#and then applying these components as c predictors in our linear regression model. They key premise here is that a small number\n",
    "#of principal components are enough to explain the variance in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Machine Learning\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
