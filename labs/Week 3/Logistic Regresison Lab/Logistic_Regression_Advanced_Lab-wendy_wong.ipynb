{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (optional advanced lab)\n",
    "\n",
    "Logistic regression models $P(y|\\boldsymbol{x})$ directly by assuming it is a (logistic) function of a linear combination of the features. The logistic function $\\theta(s) = \\frac{e^s}{e^s+1}$ maps the weighted features to $[0,1]$ to allow it to model a probability. Training logistic regression corresponds to learning the weights $\\boldsymbol{w}$ to maximise the likelihood function:\n",
    "\n",
    "\\begin{equation}\n",
    "P(y_1...y_n|\\boldsymbol{x}_1...\\boldsymbol{x}_n,\\boldsymbol{w}) = \\prod_{i=1}^n \\theta(y_i\\boldsymbol{w}^T\\boldsymbol{x}_i)\n",
    "\\end{equation}\n",
    "\n",
    "Maximising the likelihood $P(y_1...y_n|\\boldsymbol{x}_1...\\boldsymbol{x}_n,\\boldsymbol{w})$ is equivalent to minimising the negative log-likelihood: \n",
    "\\begin{equation}\n",
    "\\boldsymbol{w}^* = argmin_{\\boldsymbol{w}}\\left( -\\log\\left(\\prod_{i=1}^n \\theta(y_i\\boldsymbol{w}^T\\boldsymbol{x}_i)\\right)\\right)\n",
    "= argmin_{\\boldsymbol{w}}\\left( \\sum_{i=1}^n \\ln(1+e^{-y_i\\boldsymbol{w}^T\\boldsymbol{x}_i})\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Once we have the weights $\\boldsymbol{w}^*$, we can predict the probability that a new observation belongs to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): python-utils in /Users/wendywong/Documents/anaconda/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): six in /Users/wendywong/Documents/anaconda/lib/python2.7/site-packages (from python-utils)\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import python_utils\n",
    "\n",
    "from scipy.stats import itemfreq\n",
    "from scipy.stats import bernoulli\n",
    "from scipy.stats import multivariate_normal as mvnorm\n",
    "\n",
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib\n",
    "print(\"SKLEARN\",sklearn.__version__)\n",
    "print (\"SCIPY\",scipy.version.full_version)\n",
    "print(\"NUMPY\",np.__version__)\n",
    "print(\"MATPLOTLIB\",matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise: Use Sklearn to fit a logistic regression model on the gaussian mixture data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `mixture` not found.\n"
     ]
    }
   ],
   "source": [
    "?mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'util' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-173085407e04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcov0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m.28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# the covariance matrix for class 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcov1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m.8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# the covariance matrix for class 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmixture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGaussianMixture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcov0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcov1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'util' is not defined"
     ]
    }
   ],
   "source": [
    "# Generates some data from a Gaussian Mixture Model. \n",
    "mean0 = [-1,-1]  # the mean of the gaussian for class 0      \n",
    "mean1 = [1,1] # the mean of the gaussian for class 1\n",
    "cov0 = [[.5, .28], [.28, .5]] # the covariance matrix for class 0\n",
    "cov1 = [[1, -.8], [-.8, 1]] # the covariance matrix for class 1\n",
    "mixture = util.GaussianMixture(mean0,cov0,mean1,cov1)\n",
    "mX,mY = mixture.sample(500,0.5,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-aca5ac9a0b48>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-aca5ac9a0b48>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    logistic = YOUR CODE HERE # create and fit a logistic regression model on mX,mY\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Run Logistic regression on the gaussian mixture data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic = YOUR CODE HERE # create and fit a logistic regression model on mX,mY\n",
    "\n",
    "# print out the intercept and coefficients, w\n",
    "YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logistic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7f85de00906b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plot the probability y = 1 as over the feature space as for Naive Bayes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogistz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# probability Y = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontourf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogistz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logistic' is not defined"
     ]
    }
   ],
   "source": [
    "# plot the probability y = 1 as over the feature space as for Naive Bayes\n",
    "logistz = logistic.predict_proba(test_points)[:,1].reshape(len(x),len(y)) # probability Y = 1\n",
    "f,ax = subplots(1,1,figsize=(5,5))\n",
    "cn = ax.contourf(x,y,logistz)\n",
    "ct = ax.contour(cn,levels=[0.5])\n",
    "ax.scatter(mX[:,0],mX[:,1],s=5, c = [\"black\" if t < 1 else \"white\" for t in mY],alpha=1)\n",
    "ax.clabel(ct)\n",
    "show()# implement the jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-b33e9ab0aa44>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-b33e9ab0aa44>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    poly_expand = YOUR CODE HERE # create a polynomial feature transformer that produces quadratic combinations\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# we can model more complex decision boundaries by expanding the feature space to include combinations of features\n",
    "\n",
    "# re-fit logistic regression adding in all quadratic combinations of features ie x1,x2,x1x2,x1^2,x2^2\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_expand = YOUR CODE HERE # create a polynomial feature transformer that produces quadratic combinations\n",
    "m2X = YOUR CODE HERE # use poly_expand to transform the original features (mX)\n",
    "logistic.YOUR CODE HERE # fit the logistic model with the new features\n",
    "\n",
    "# transform the test plots and predict and plot\n",
    "testpoints2 = poly_expand.transform(test_points)\n",
    "logistic2z = logistic.predict_proba(testpoints2)[:,1].reshape(len(x),len(y)) # probability Y = 1\n",
    "f,ax = subplots(1,1,figsize=(5,5))\n",
    "cn = ax.contourf(x,y,logistic2z)\n",
    "ct = ax.contour(cn,levels=[0.5])\n",
    "ax.scatter(mX[:,0],mX[:,1],s=5, c = [\"black\" if t < 1 else \"white\" for t in mY],alpha=1)\n",
    "ax.clabel(ct)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With large numbers of features there is a risk of over fitting to the training data. We can tune a logistic regression model to reduce the risk of over fitting by penalising large weights, $\\boldsymbol{w}$ \n",
    "\n",
    "*Exersise: Experiment with the regularisation parameters sklearn provides: \n",
    "penalty = \"l1\" or \"l2\" and C = inverse of weight of regularisation term *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-0d4684634f80>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-0d4684634f80>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    lreg = YOUR CODE HERE # create and fit a logistic regression model to the quadraticly expanded features\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "lreg = YOUR CODE HERE # create and fit a logistic regression model to the quadraticly expanded features\n",
    "\n",
    "\n",
    "# plots the probability as before\n",
    "logistic2z_reg = lreg.predict_proba(testpoints2)[:,1].reshape(len(x),len(y)) # probability Y = 1\n",
    "f,ax = subplots(1,1,figsize=(5,5))\n",
    "cn = ax.contourf(x,y,logistic2z_reg)\n",
    "ct = ax.contour(cn,levels=[0.5])\n",
    "ax.scatter(mX[:,0],mX[:,1],s=5, c = [\"black\" if t < 1 else \"white\" for t in mY],alpha=1)\n",
    "ax.clabel(ct)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
