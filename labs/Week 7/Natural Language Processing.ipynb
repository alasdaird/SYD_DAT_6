{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science with Python \n",
    "## Natural Language Processing (NLP)\n",
    "\n",
    "This notebook contains exercises for getting started with Natual Language Processing in Python. The main topics we will cover in this class are:\n",
    "\n",
    "Introduction\n",
    "1. newsgroups dataset\n",
    "- Bag of words prediciton model\n",
    "\n",
    "Advanced Language Processing with NLTK \n",
    "1. Tokenizing\n",
    "- Stemming\n",
    "- Speech Tagging\n",
    "- Named Entity Recognition\n",
    "- Term Frequency - Inverse Document Frequency\n",
    "- Latent Dirichlet Allocation\n",
    "- Regex\n",
    "\n",
    "Other:\n",
    "- Analysing data with the Alchemy API - http://www.alchemyapi.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Prediction\n",
    "\n",
    "We will use the [20 Newsgroup dataset](http://qwone.com/~jason/20Newsgroups/), which is provided by Scikit-Learn. This is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "We will restrict the analysis to 4 groups and will attempt to classify them starting from the corresponding text.\n",
    "\n",
    "This is a typical example of text classification, where a data scientist's task is to train a model that can partition text in pre-defined categories. Other examples include sentiment analysis and topic assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data inspection\n",
    "\n",
    "We have downloaded a few newsgroup categories and removed headers, footers and quotes.\n",
    "\n",
    "Let's inspect them.\n",
    "\n",
    "1. What data taype is `data_train`\n",
    "> sklearn.datasets.base.Bunch\n",
    "- Is it like a list? Or like a Dictionary? or what?\n",
    "> Dict\n",
    "- How many data points does it contain?\n",
    "- Inspect the first data point, what does it look like?\n",
    "> A blurb of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag of Words model\n",
    "\n",
    "Let's train a model using a simple count vectorizer\n",
    "\n",
    "[Text Feature Extraction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "1. Initialize a standard CountVectorizer and fit the training data (tokenization and occurrence counting)\n",
    "- how big is the feature dictionary\n",
    "- repeat eliminating english stop words\n",
    "- is the dictionary smaller?\n",
    "- transform the training data using the trained vectorizer\n",
    "- what are the 20 words that are most common in the whole corpus?\n",
    "- what are the 20 most common words in each of the 4 classes?\n",
    "- evaluate the performance of a Lotistic Regression on the features extracted by the CountVectorizer\n",
    "    - you will have to transform the test_set too. Be carefule to use the trained vectorizer, without re-fitting it\n",
    "- try the following 3 modification:\n",
    "    - restrict the max_features\n",
    "    - change max_df and min_df\n",
    "    - use a fixed vocabulary of size 80 combining the 20 most common words per group found earlier\n",
    "- for each of the above print a confusion matrix and investigate what gets mixed\n",
    "> Anwer: not surprisingly if we reduce the feature space we lose accuracy\n",
    "- print out the number of features for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "cvec.fit(data_train['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english')\n",
    "cvec.fit(data_train['data'])\n",
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(cvec.transform(data_train['data']).todense(),\n",
    "                       columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = data_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_counts = X_train.sum(axis=0)\n",
    "word_counts.sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = data_train['target_names']\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_words = []\n",
    "for i in xrange(4):\n",
    "    word_count = X_train[y_train==i].sum(axis=0)\n",
    "    print names[i], \"most common words\"\n",
    "    cw = word_count.sort_values(ascending = False).head(20)\n",
    "#     cw.to_csv('../../../5.2-lesson/assets/datasets/'+names[i]+'_most_common_words.csv')\n",
    "    print cw\n",
    "    common_words.extend(cw.index)\n",
    "    print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(cvec.transform(data_test['data']).todense(),\n",
    "                      columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(cvec.transform(data_test['data']).todense(),\n",
    "                      columns=cvec.get_feature_names())\n",
    "y_test = data_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docm(y_true, y_pred, labels=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if labels is not None:\n",
    "        cols = ['p_'+c for c in labels]\n",
    "        df = pd.DataFrame(cm, index=labels, columns=cols)\n",
    "    else:\n",
    "        cols = ['p_'+str(i) for i in xrange(len(cm))]\n",
    "        df = pd.DataFrame(cm, columns=cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(CountVectorizer(stop_words='english',\n",
    "                                      max_features=1000),\n",
    "                      LogisticRegression(),\n",
    "                      )\n",
    "model.fit(data_train['data'], y_train)\n",
    "y_pred = model.predict(data_test['data'])\n",
    "print accuracy_score(y_test, y_pred)\n",
    "docm(y_test, y_pred, names)\n",
    "print \"Number of features:\", len(model.steps[0][1].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docm(y_test, y_pred, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(CountVectorizer(stop_words='english',\n",
    "                                      max_features=2000,\n",
    "                                      min_df=0.01),\n",
    "                      LogisticRegression(),\n",
    "                      )\n",
    "model.fit(data_train['data'], y_train)\n",
    "y_pred = model.predict(data_test['data'])\n",
    "print accuracy_score(y_test, y_pred)\n",
    "docm(y_test, y_pred, names)\n",
    "print \"Number of features:\", len(model.steps[0][1].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(CountVectorizer(stop_words='english',\n",
    "                                      vocabulary=set(common_words)),\n",
    "                      LogisticRegression(),\n",
    "                      )\n",
    "model.fit(data_train['data'], y_train)\n",
    "y_pred = model.predict(data_test['data'])\n",
    "print accuracy_score(y_test, y_pred)\n",
    "docm(y_test, y_pred, names)\n",
    "print \"Number of features:\", len(model.steps[0][1].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Natural Language Processing with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following step, see the new window that will pop out elsewhere. The whole NLTK package is huge. Too big for all of you to download in class at the same time. Instead, select to install just the main packages and see of the lab runs. If not, use the downloader to get the additional missing packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find pop-up window for downloader tool\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "What:  Separate text into units such as sentences or words\n",
    "\n",
    "Why:   Gives structure to previously unstructured text\n",
    "\n",
    "Notes: Relatively easy with English language text, not easy with some languages\n",
    "\n",
    "\n",
    "\"corpus\" = collection of documents\n",
    "\n",
    "\"corpora\" = plural form of corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Data science is an interdisciplinary field about processes and systems to extract knowledge or insights from data in various forms, either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, machine learning, data mining, and predictive analytics,[3] similar to Knowledge Discovery in Databases (KDD).   Data science employs techniques and theories drawn from many fields within the broad areas of mathematics, statistics, operations rese'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get(\"http://en.wikipedia.org/wiki/Data_science\")\n",
    "b = BeautifulSoup(r.text, \"lxml\")\n",
    "paragraphs = b.find(\"body\").findAll(\"p\")\n",
    "text = \"\"\n",
    "for paragraph in paragraphs:\n",
    "    text += paragraph.text + \" \"\n",
    "# Data Science corpus\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Data science is an interdisciplinary field about processes and systems to extract knowledge or insights from data in various forms, either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, machine learning, data mining, and predictive analytics,[3] similar to Knowledge Discovery in Databases (KDD).',\n",
       " u'Data science employs techniques and theories drawn from many fields within the broad areas of mathematics, statistics, operations research,[4] information science, and computer science, including signal processing, probability models, machine learning, statistical learning, data mining, database, data engineering, pattern recognition and learning, visualization, predictive analytics, uncertainty modeling, data warehousing, data compression, computer programming, artificial intelligence, and high performance computing.',\n",
       " u'Methods that scale to big data are of particular interest in data science, although the discipline is not generally considered to be restricted to such big data, and big data technologies are often focused on organizing and preprocessing the data instead of analysis.',\n",
       " u'The development of machine learning has enhanced the growth and importance of data science.',\n",
       " u'Data science affects academic and applied research in many domains, including machine translation, speech recognition, robotics, search engines, digital economy, but also the biological sciences, medical informatics, health care, social sciences and the humanities.',\n",
       " u'It heavily influences economics, business and finance.',\n",
       " u'From the business perspective, data science is an integral part of competitive intelligence, a newly emerging field that encompasses a number of activities, such as data mining and data analysis.',\n",
       " u'[5] Data scientists use their data and analytical ability to find and interpret rich data sources; manage large amounts of data despite hardware, software, and bandwidth constraints; merge data sources; ensure consistency of datasets; create visualizations to aid in understanding data; build mathematical models using the data; and present and communicate the data insights/findings.',\n",
       " u'They are often expected to produce answers in days rather than months, work by exploratory analysis and rapid iteration, and to produce and present results with dashboards (displays of current values) rather than papers/reports, as statisticians normally do.',\n",
       " u'[6] \"Data Scientist\" has become a popular occupation with Harvard Business Review dubbing it \"The Sexiest Job of the 21st Century\" [7] and McKinsey & Company projecting a global excess demand of 1.5 million new data scientists.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize into sentences\n",
    "sentences = [sent for sent in nltk.sent_tokenize(text)]\n",
    "sentences[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Data',\n",
       " u'science',\n",
       " u'is',\n",
       " u'an',\n",
       " u'interdisciplinary',\n",
       " u'field',\n",
       " u'about',\n",
       " u'processes',\n",
       " u'and',\n",
       " u'systems',\n",
       " u'to',\n",
       " u'extract',\n",
       " u'knowledge',\n",
       " u'or',\n",
       " u'insights',\n",
       " u'from',\n",
       " u'data',\n",
       " u'in',\n",
       " u'various',\n",
       " u'forms',\n",
       " u',',\n",
       " u'either',\n",
       " u'structured',\n",
       " u'or',\n",
       " u'unstructured',\n",
       " u',',\n",
       " u'[',\n",
       " u'1',\n",
       " u']',\n",
       " u'[',\n",
       " u'2',\n",
       " u']',\n",
       " u'which',\n",
       " u'is',\n",
       " u'a',\n",
       " u'continuation',\n",
       " u'of',\n",
       " u'some',\n",
       " u'of',\n",
       " u'the',\n",
       " u'data',\n",
       " u'analysis',\n",
       " u'fields',\n",
       " u'such',\n",
       " u'as',\n",
       " u'statistics',\n",
       " u',',\n",
       " u'machine',\n",
       " u'learning',\n",
       " u',',\n",
       " u'data',\n",
       " u'mining',\n",
       " u',',\n",
       " u'and',\n",
       " u'predictive',\n",
       " u'analytics',\n",
       " u',',\n",
       " u'[',\n",
       " u'3',\n",
       " u']',\n",
       " u'similar',\n",
       " u'to',\n",
       " u'Knowledge',\n",
       " u'Discovery',\n",
       " u'in',\n",
       " u'Databases',\n",
       " u'(',\n",
       " u'KDD',\n",
       " u')',\n",
       " u'.',\n",
       " u'Data',\n",
       " u'science',\n",
       " u'employs',\n",
       " u'techniques',\n",
       " u'and',\n",
       " u'theories',\n",
       " u'drawn',\n",
       " u'from',\n",
       " u'many',\n",
       " u'fields',\n",
       " u'within',\n",
       " u'the',\n",
       " u'broad',\n",
       " u'areas',\n",
       " u'of',\n",
       " u'mathematics',\n",
       " u',',\n",
       " u'statistics',\n",
       " u',',\n",
       " u'operations',\n",
       " u'research',\n",
       " u',',\n",
       " u'[',\n",
       " u'4',\n",
       " u']',\n",
       " u'information',\n",
       " u'science',\n",
       " u',',\n",
       " u'and',\n",
       " u'computer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize into words\n",
    "tokens = [word for word in nltk.word_tokenize(text)]\n",
    "tokens[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Data',\n",
       " u'science',\n",
       " u'is',\n",
       " u'an',\n",
       " u'interdisciplinary',\n",
       " u'field',\n",
       " u'about',\n",
       " u'processes',\n",
       " u'and',\n",
       " u'systems',\n",
       " u'to',\n",
       " u'extract',\n",
       " u'knowledge',\n",
       " u'or',\n",
       " u'insights',\n",
       " u'from',\n",
       " u'data',\n",
       " u'in',\n",
       " u'various',\n",
       " u'forms',\n",
       " u'either',\n",
       " u'structured',\n",
       " u'or',\n",
       " u'unstructured',\n",
       " u'which',\n",
       " u'is',\n",
       " u'a',\n",
       " u'continuation',\n",
       " u'of',\n",
       " u'some',\n",
       " u'of',\n",
       " u'the',\n",
       " u'data',\n",
       " u'analysis',\n",
       " u'fields',\n",
       " u'such',\n",
       " u'as',\n",
       " u'statistics',\n",
       " u'machine',\n",
       " u'learning',\n",
       " u'data',\n",
       " u'mining',\n",
       " u'and',\n",
       " u'predictive',\n",
       " u'analytics',\n",
       " u'similar',\n",
       " u'to',\n",
       " u'Knowledge',\n",
       " u'Discovery',\n",
       " u'in',\n",
       " u'Databases',\n",
       " u'KDD',\n",
       " u'Data',\n",
       " u'science',\n",
       " u'employs',\n",
       " u'techniques',\n",
       " u'and',\n",
       " u'theories',\n",
       " u'drawn',\n",
       " u'from',\n",
       " u'many',\n",
       " u'fields',\n",
       " u'within',\n",
       " u'the',\n",
       " u'broad',\n",
       " u'areas',\n",
       " u'of',\n",
       " u'mathematics',\n",
       " u'statistics',\n",
       " u'operations',\n",
       " u'research',\n",
       " u'information',\n",
       " u'science',\n",
       " u'and',\n",
       " u'computer',\n",
       " u'science',\n",
       " u'including',\n",
       " u'signal',\n",
       " u'processing',\n",
       " u'probability',\n",
       " u'models',\n",
       " u'machine',\n",
       " u'learning',\n",
       " u'statistical',\n",
       " u'learning',\n",
       " u'data',\n",
       " u'mining',\n",
       " u'database',\n",
       " u'data',\n",
       " u'engineering',\n",
       " u'pattern',\n",
       " u'recognition',\n",
       " u'and',\n",
       " u'learning',\n",
       " u'visualization',\n",
       " u'predictive',\n",
       " u'analytics',\n",
       " u'uncertainty',\n",
       " u'modeling',\n",
       " u'data']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only keep tokens that start with a letter (using regular expressions)\n",
    "import re\n",
    "clean_tokens = [token for token in tokens if re.search('^[a-zA-Z]+', token)]\n",
    "clean_tokens[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', 56),\n",
       " (u'data', 51),\n",
       " (u'the', 50),\n",
       " (u'of', 41),\n",
       " (u'science', 25),\n",
       " (u'in', 22),\n",
       " (u'to', 19),\n",
       " (u'Data', 18),\n",
       " (u'a', 16),\n",
       " (u'In', 14),\n",
       " (u'Science', 12),\n",
       " (u'for', 11),\n",
       " (u'as', 10),\n",
       " (u'is', 10),\n",
       " (u'The', 8),\n",
       " (u'term', 8),\n",
       " (u'on', 8),\n",
       " (u'his', 7),\n",
       " (u'are', 7),\n",
       " (u'software', 7),\n",
       " (u'analysis', 6),\n",
       " (u'was', 6),\n",
       " (u'scientists', 5),\n",
       " (u'such', 5),\n",
       " (u'their', 5)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the tokens\n",
    "from collections import Counter\n",
    "c = Counter(clean_tokens)\n",
    "\n",
    "c.most_common(25)       # mixed case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action 1\n",
      "Advanced 2\n",
      "Although 1\n",
      "American 1\n",
      "An 1\n",
      "Analytics 3\n",
      "April 2\n",
      "Areas 1\n",
      "Assembly 1\n",
      "Association 1\n",
      "Board 1\n",
      "Business 1\n",
      "C. 1\n",
      "C.F 1\n",
      "CODATA 1\n",
      "Carver 1\n",
      "Century 2\n",
      "Chandra 1\n",
      "Classification 1\n",
      "Cleveland 2\n",
      "Collections 1\n",
      "Columbia 1\n",
      "Committee 1\n",
      "Company 1\n",
      "Computer 1\n"
     ]
    }
   ],
   "source": [
    "sorted(c.items())[:25]  # counts similar words separately\n",
    "for item in sorted(c.items())[:25]:\n",
    "    print item[0], item[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################\n",
    "##### EXERCISE ####\n",
    "###################\n",
    "\n",
    "# Put each word in clean_tokens in lower case\n",
    "# find the new word count of the lowered tokens\n",
    "# Then show the top 10 words used in this corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "What:  Reduce a word to its base/stem form\n",
    "\n",
    "Why:   Often makes sense to treat multiple word forms the same way\n",
    "\n",
    "Notes: Uses a \"simple\" and fast rule-based approach\n",
    "       Output can be undesirable for irregular words\n",
    "       Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "       Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'charg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example stemming\n",
    "stemmer.stem('charge')\n",
    "stemmer.stem('charging')\n",
    "stemmer.stem('charged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stem the tokens\n",
    "stemmed_tokens = [stemmer.stem(t) for t in clean_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count the stemmed tokens\n",
    "c = Counter(stemmed_tokens)\n",
    "c.most_common(25)       # all lowercase\n",
    "sorted(c.items())[:25]  # some are strange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "What:  Derive the canonical form ('lemma') of a word\n",
    "    \n",
    "Why:   Can be better than stemming, reduces words to a 'normal' form.\n",
    "    \n",
    "Notes: Uses a dictionary-based approach (slower than stemming)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'dog'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "# compare stemmer to lemmatizer\n",
    "stemmer.stem('dogs')\n",
    "lemmatizer.lemmatize('dogs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer.stem('wolves') # Beter for information retrieval and search\n",
    "lemmatizer.lemmatize('wolves') # Better for text analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer.stem('is')\n",
    "lemmatizer.lemmatize('is')\n",
    "lemmatizer.lemmatize('is',pos='v')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging\n",
    "\n",
    "What:  Determine the part of speech of a word\n",
    "    \n",
    "Why:   This can inform other methods and models such as Named Entity Recognition\n",
    "    \n",
    "Notes: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('revision', 'NN'),\n",
       " ('class', 'NN'),\n",
       " ('this', 'DT'),\n",
       " ('Saturday', 'NNP')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_sent = 'We have a revision class this Saturday'\n",
    "# pos_tag takes a tokenize sentence\n",
    "nltk.pos_tag(nltk.word_tokenize(temp_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "\n",
    "What:  Remove common words that will likely appear in any text\n",
    "    \n",
    "Why:   They don't tell you much about your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# most of top 25 stemmed tokens are \"worthless\"\n",
    "c.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# view the list of stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "sorted(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "### Exercise  ####\n",
    "##################\n",
    "\n",
    "\n",
    "# Create a variable called stemmed_stops which is the \n",
    "# stemmed version of each stopword in stopwords\n",
    "# Use the stemmer we used up above!\n",
    "\n",
    "# Then create a list called stemmed_tokens_no_stop that \n",
    "# contains only the tokens in stemmed_tokens that aren't in \n",
    "# stemmed_stops\n",
    "\n",
    "# Show the 25 most common stemmed non stop word tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "What:  Automatically extract the names of people, places, organizations, etc.\n",
    "\n",
    "Why:   Can help you to identify \"important\" words\n",
    "\n",
    "Notes: Training NER classifier requires a lot of annotated training data\n",
    "       Should be trained on data relevant to your task\n",
    "       Stanford NER classifier is the \"gold standard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alasdair', 'is', 'an', 'instructor', 'for', 'General', 'Assembly']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Alasdair is an instructor for General Assembly'\n",
    "\n",
    "tokenized = nltk.word_tokenize(sentence)\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alasdair', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('instructor', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('General', 'NNP'),\n",
       " ('Assembly', 'NNP')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "tagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('GPE', [('Alasdair', 'NNP')]), ('is', 'VBZ'), ('an', 'DT'), ('instructor', 'NN'), ('for', 'IN'), Tree('ORGANIZATION', [('General', 'NNP'), ('Assembly', 'NNP')])])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = nltk.ne_chunk(tagged)\n",
    "\n",
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPE] Alasdair\n",
      "[ORGANIZATION] General Assembly\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(text):\n",
    "    entities = []\n",
    "    # tokenize into sentences\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        # tokenize sentences into words\n",
    "        # add part-of-speech tags\n",
    "        # use NLTK's NER classifier\n",
    "        chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "        # parse the results\n",
    "        entities.extend([chunk for chunk in chunks if hasattr(chunk, 'label')])\n",
    "    return entities\n",
    "\n",
    "for entity in extract_entities('Alasdair is an instructor for General Assembly'):\n",
    "    print '[' + entity.label() + '] ' + ' '.join(c[0] for c in entity.leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "What:  Computes \"relative frequency\" that a word appears in a document\n",
    "           compared to its frequency across all documents\n",
    "\n",
    "Why:   More useful than \"term frequency\" for identifying \"important\" words in\n",
    "           each document (high frequency in that document, low frequency in\n",
    "           other documents)\n",
    "\n",
    "Notes: Used for search engine scoring, text summarization, document clustering\n",
    "\n",
    "How: \n",
    "    TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "    IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = ['Bob likes sports', 'Bob hates sports', 'Bob likes likes trees']\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Each row represents a sentence\n",
    "# Each column represents a word\n",
    "vect.fit_transform(sample).toarray()\n",
    "vect.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(sample).toarray()\n",
    "tfidf.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the IDF of each word\n",
    "idf = tfidf.idf_\n",
    "print dict(zip(tfidf.get_feature_names(), idf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "## Exercise ###\n",
    "###############\n",
    "\n",
    "\n",
    "# for each sentence in sample, find the most \"interesting \n",
    "#words\" by ordering their tfidf in ascending order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA - Latent Dirichlet Allocation\n",
    "\n",
    "What:  Way of automatically discovering topics from sentences\n",
    "\n",
    "Why:   Much quicker than manually creating and identifying topic clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lda\n",
    "\n",
    "# Instantiate a count vectorizer with two additional parameters\n",
    "vect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \n",
    "sentences_train = vect.fit_transform(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate an LDA model\n",
    "model = lda.LDA(n_topics=10, n_iter=500)\n",
    "model.fit(sentences_train) # Fit the model \n",
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vect.get_feature_names())[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ', '.join(topic_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXAMPLE: Automatically summarize a document\n",
    "\n",
    "\n",
    "# corpus of 2000 movie reviews\n",
    "from nltk.corpus import movie_reviews\n",
    "reviews = [movie_reviews.raw(filename) for filename in movie_reviews.fileids()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create document-term matrix\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "dtm = tfidf.fit_transform(reviews)\n",
    "features = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the most and least \"interesting\" sentences in a randomly selected review\n",
    "def summarize():\n",
    "    \n",
    "    # choose a random movie review    \n",
    "    review_id = np.random.randint(0, len(reviews))\n",
    "    review_text = reviews[review_id]\n",
    "\n",
    "    # we are going to score each sentence in the review for \"interesting-ness\"\n",
    "    sent_scores = []\n",
    "    # tokenize document into sentences\n",
    "    for sentence in nltk.sent_tokenize(review_text):\n",
    "        # exclude short sentences\n",
    "        if len(sentence) > 6:\n",
    "            score = 0\n",
    "            token_count = 0\n",
    "            # tokenize sentence into words\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            # compute sentence \"score\" by summing TFIDF for each word\n",
    "            for token in tokens:\n",
    "                if token in features:\n",
    "                    score += dtm[review_id, features.index(token)]\n",
    "                    token_count += 1\n",
    "            # divide score by number of tokens\n",
    "            sent_scores.append((score / float(token_count + 1), sentence))\n",
    "\n",
    "    # lowest scoring sentences\n",
    "    print '\\nLOWEST:\\n'\n",
    "    for sent_score in sorted(sent_scores)[:3]:\n",
    "        print sent_score[1]\n",
    "\n",
    "    # highest scoring sentences\n",
    "    print '\\nHIGHEST:\\n'\n",
    "    for sent_score in sorted(sent_scores, reverse=True)[:3]:\n",
    "        print sent_score[1]\n",
    "\n",
    "# try it out!\n",
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TextBlob Demo: \"Simplified Text Processing\"\n",
    "# Installation: pip install textblob\n",
    "! pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# identify words and noun phrases\n",
    "blob = TextBlob('Liam and Sinan are instructors for General Assembly')\n",
    "blob.words\n",
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sentiment analysis\n",
    "blob = TextBlob('I hate this horrible movie. This movie is not very good.')\n",
    "blob.sentences\n",
    "blob.sentiment.polarity\n",
    "[sent.sentiment.polarity for sent in blob.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sentiment subjectivity\n",
    "TextBlob(\"I am a cool person\").sentiment.subjectivity # Pretty subjective\n",
    "TextBlob(\"I am a person\").sentiment.subjectivity # Pretty objective\n",
    "# different scores for essentially the same sentence\n",
    "print TextBlob('Ian and Alasdair are instructors for General Assembly in Sydney').sentiment.subjectivity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# singularize and pluralize\n",
    "blob = TextBlob('Put away the dishes.')\n",
    "[word.singularize() for word in blob.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[word.pluralize() for word in blob.words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spelling correction\n",
    "blob = TextBlob('15 minuets late')\n",
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spellcheck\n",
    "Word('parot').spellcheck()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# definitions\n",
    "Word('bank').define()\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# translation and language identification\n",
    "blob = TextBlob('Welcome to the classroom.')\n",
    "blob.translate(to='es')\n",
    "blob = TextBlob('Hola amigos')\n",
    "blob.detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 7. Regular Expressions - Regex\n",
    "\n",
    "This is the python module for regular expressions: https://docs.python.org/2/library/re.html\n",
    "\n",
    "Here is a google page for explaining regular expression patterns: https://developers.google.com/edu/python/regular-expressions\n",
    "\n",
    "And here is a convenient tool for testing regular expressions: https://regex101.com/#python\n",
    "\n",
    "Have a read of these and play around with regular expressions below and in the regex101 tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
<<<<<<< HEAD
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
=======
  "kernelspec": {
   "display_name": "Python 2",
>>>>>>> upstream/master
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
<<<<<<< HEAD
   "version": "2.7.12"
=======
   "version": "2.7.11"
>>>>>>> upstream/master
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
